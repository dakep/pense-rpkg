[{"path":"/articles/computing_adapense.html","id":"computing-adaptive-pense-estimates","dir":"Articles","previous_headings":"","what":"Computing adaptive PENSE estimates","title":"Estimating predictive models","text":"First need load pense package: Computing robust, regularized estimates high-dimensional linear regression models can take long time. save time, many steps can done parallel. computer 1 CPU core, can harness computing power creating cluster R processes: guide uses following simulated data 50 observations 40 available predictors. error distribution heavy-tailed t-distribution first 3 predictors truly relevant predicting response y: make scenario realistic, let’s add contamination response value first 3 observations predictors:","code":"library(pense) #> Loading required package: Matrix library(parallel) # If you don't know how many CPU cores are available, first run `detectCores(logical = FALSE)` cluster <- makeCluster(2) set.seed(1234) x <- matrix(rweibull(50 * 40, 2, 3), ncol = 40) y <- 1 * x[, 1] - 1.1 * x[, 2] + 1.2 * x[, 3] + rt(nrow(x), df = 2) y[1:3] <- 5 * apply(x[1:3, ], 1, max) x[3:6, 4:6] <- 1.5 * max(x) + abs(rcauchy(4 * 3))"},{"path":"/articles/computing_adapense.html","id":"step-1-computing-the-estimates","dir":"Articles","previous_headings":"Computing adaptive PENSE estimates","what":"Step 1: Computing the estimates","title":"Estimating predictive models","text":"first step compute adaptive PENSE estimates fixed value hyper-parameter alpha many different penalization levels (hyper-parameter lambda). adapense_cv() function automatically determines grid penalization levels, parameter nlambda= controlling number different penalization levels used (default 50). going choose penalization level leads good balance prediction accuracy model size. pense package can automatically evaluate prediction accuracy adaptive PENSE estimates via robust information-sharing cross-validation (RIS-CV; Kepplinger & Wei 2025). simplest form, computing adaptive PENSE estimates estimating prediction accuracy done code . , prediction accuracy estimated via 5-fold RIS-CV, replicated 3 times: always set seed prior computing adaptive PENSE PENSE estimates ensure reproducibility internal cross-validation. default, adaptive PENSE estimates computed breakdown point ~25% Tukey’s bisquare \\rho function. means, estimates reliable 25% observations contain arbitrary contamination. suspect larger proportion observations may affected contamination, can increase breakdown point estimates argument bdp= 50%. Note, however, higher breakdown point also leads less accurate estimates. package also supports “optimal” \\rho function (Maronna et al., 2018, Section 5.8.1). choose different \\rho function, alter mscale_opts argument:","code":"set.seed(1234) fit_075 <- adapense_cv(x, y, alpha = 0.75, cv_k = 5, cv_repl = 3, cl = cluster) set.seed(1234) fit_075_mopt <- adapense_cv(x, y, alpha = 0.75, cv_k = 5, cv_repl = 3, cl = cluster,                             mscale_opts = mscale_algorithm_options(rho = \"mopt\"))"},{"path":"/articles/computing_adapense.html","id":"step-2-assessing-prediction-performance","dir":"Articles","previous_headings":"Computing adaptive PENSE estimates","what":"Step 2: Assessing prediction performance","title":"Estimating predictive models","text":"plot() function object fit_075 shows estimated prediction accuracy fitted models. Estimated prediction accuracy using 3 replications 5-fold CV. plot shows estimated scale prediction error 50 models. penalization level leading best prediction performance highlighted dark blue dot. one CV replication performed, plot also shows light blue dot, marking parsimonious model prediction performance “indistinguishable” best model. plot uses “one-standard-error” rule using minimum average scale prediction error plus 1 standard error estimated scale. can adjust rule “m-standard-error” plot(fit_075, se_mult = m), m positive number (e.g., m=2). accuracy estimate can improved increasing number CV replications. CV replications, accurate estimates prediction accuracy, longer computing time. repeat step #1, 10 CV replications instead 3, get stable evaluation prediction performance: Estimated prediction accuracy using 10 replications 5-fold CV.","code":"plot(fit_075) set.seed(1234) fit_075 <- adapense_cv(x, y, alpha = 0.75, cv_k = 5, cv_repl = 10, cl = cluster)"},{"path":"/articles/computing_adapense.html","id":"step-3-extracting-coefficients","dir":"Articles","previous_headings":"Computing adaptive PENSE estimates","what":"Step 3: Extracting coefficients","title":"Estimating predictive models","text":"happy stability estimated prediction performance, can extract summary information predictive model model corresponds model smallest scale prediction error (blue dot plot ). total 6 predictors model. think sparser model may appropriate application, can also apply m-standard-error rule plots. default, one-standard-error rule leads following predictive model: fit, 4 40 predictors relevant, including 3 truly relevant predictors. maybe different values hyper-parameters alpha exponent lead even better prediction?","code":"summary(fit_075) #> Adaptive PENSE fit with prediction performance estimated by 10 replications of  #> 5-fold ris cross-validation. #>  #> 6 out of 40 predictors have non-zero coefficients: #>  #>                Estimate #> (Intercept)  0.89610051 #> X1           0.66483511 #> X2          -0.73465818 #> X3           0.55847642 #> X5           0.03486861 #> X29          0.03892034 #> X39         -0.01298290 #> --- #>  #> Hyper-parameters: lambda=0.1566381, alpha=0.75, exponent=1 summary(fit_075, lambda = \"se\", se_mult = 1) #> Adaptive PENSE fit with prediction performance estimated by 10 replications of  #> 5-fold ris cross-validation. #>  #> 4 out of 40 predictors have non-zero coefficients: #>  #>                Estimate #> (Intercept)  1.28199718 #> X1           0.56993574 #> X2          -0.68590414 #> X3           0.46982332 #> X5           0.02356945 #> --- #>  #> Hyper-parameters: lambda=0.1814144, alpha=0.75, exponent=1"},{"path":"/articles/computing_adapense.html","id":"step-4-exploring-different-hyper-parameters","dir":"Articles","previous_headings":"Computing adaptive PENSE estimates","what":"Step 4: Exploring different hyper-parameters","title":"Estimating predictive models","text":"choice hyper-parameters alpha exponent (kept default value 1) rather arbitrary. effects two hyper-parameters estimates general less pronounced penalization level. may still want explore different values alpha exponent. alpha=0.75 good value many applications, alpha=1 may also interest, particularly applications correlation predictors issue. applications high correlation predictors, lower values alpha (e.g., alpha=0.5) may lead stability variable selection. hyper-parameter exponent generally effect sparsity models. higher values exponent, typically predictors largest (absolute magnitude) standardized coefficients non-zero. helps screen many truly irrelevant, also risks missing truly relevant predictors. Let us compute adaptive PENSE estimates different values hyper-parameters alpha exponent. code , done two values: alpha=0.75 alpha=1 well exponent=1 exponent=2. Note set seed call adapense_cv(). ensure reproducibility CV, also make estimated prediction performance comparable across different values exponent hyper-parameter. checking RIS-CV prediction performance fitted models smooth enough reliably select penalization level, can compare estimates. , package includes function prediction_performance(), extracts prints prediction performance given objects: see combination hyper-parameters alpha=1 exponent=2 (object fit_exp_2) leads best prediction accuracy. can also compare sparser models similar prediction performance: interested even sparser models, can increase tolerance level , instance, 2 standard errors: relaxation now leads combination hyper-parameters alpha=0.75 exponent=2 3 relevant predictors. can see estimated coefficients estimated relevant predictors close truth:","code":"set.seed(1234) fit_exp_1 <- adapense_cv(x, y, alpha = c(0.75, 1), exponent = 1, cv_k = 5, cv_repl = 10, cl = cluster)  set.seed(1234) fit_exp_2 <- adapense_cv(x, y, alpha = c(0.75, 1), exponent = 2, cv_k = 5, cv_repl = 10, cl = cluster) prediction_performance(fit_exp_1, fit_exp_2) #> Prediction performance estimated by cross-validation: #>  #>       Model Estimate Std. Error Predictors alpha exp. #> 1 fit_exp_2 1.440873 0.09734518          4  1.00    2 #> 2 fit_exp_2 1.451103 0.09829709          7  0.75    2 #> 3 fit_exp_1 1.553635 0.13786022          4  1.00    1 #> 4 fit_exp_1 1.562786 0.13463294          6  0.75    1 prediction_performance(fit_exp_1, fit_exp_2, lambda = 'se') #> Prediction performance estimated by cross-validation: #>  #>       Model Estimate Std. Error Predictors alpha exp. #> 1 fit_exp_2 1.498405 0.08330122          3  0.75    2 #> 2 fit_exp_2 1.517282 0.11277732          3  1.00    2 #> 3 fit_exp_1 1.603343 0.13706756          4  0.75    1 #> 4 fit_exp_1 1.628196 0.14851228          3  1.00    1 prediction_performance(fit_exp_1, fit_exp_2, lambda = 'se', se_mult = 2) #> Prediction performance estimated by cross-validation: #>  #>       Model Estimate Std. Error Predictors alpha exp. #> 1 fit_exp_2 1.561675  0.1044480          3  0.75    2 #> 2 fit_exp_2 1.607036  0.1202396          3  1.00    2 #> 3 fit_exp_1 1.754913  0.1452938          3  1.00    1 #> 4 fit_exp_1 1.823087  0.1272765          4  0.75    1 summary(fit_exp_2, alpha = 0.75, lambda = 'se') #> Adaptive PENSE fit with prediction performance estimated by 10 replications of  #> 5-fold ris cross-validation. #>  #> 3 out of 40 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept)  1.1507826 #> X1           0.7821125 #> X2          -0.8275043 #> X3           0.4945724 #> --- #>  #> Hyper-parameters: lambda=0.02282, alpha=0.75, exponent=2"},{"path":"/articles/computing_adapense.html","id":"using-different-measures-of-prediction-performance","dir":"Articles","previous_headings":"","what":"Using different measures of prediction performance","title":"Estimating predictive models","text":"default, adapense_cv() uses robust information sharing (RIS) cross-validation procedure corresponding weighted mean squared prediction error estimate prediction accuracy. user can choose use standard CV via adapense_cv(cv_type = \"naive\"), also enables user choose different metrics prediction performance. package supports tau-scale estimate prediction error (default naïve CV), mean absolute prediction error (cv_metric = \"mape\") classical root mean squared prediction error (cv_metric = \"rmspe\"). , however, use RMSPE evaluate prediction performance potential presence contamination. Robust methods designed predict contaminated observations well RMSPE may artificially inflated poor prediction contaminated response values. can also specify function takes input vector prediction errors returns single number, measuring prediction performance. example, use mean absolute prediction error, write matrix estimates prediction performance accessible slot $cvres object returned adapense_cv(). rows correspond different penalization levels.","code":"mae <- function (prediction_errors) {   mean(abs(prediction_errors)) }  set.seed(1234) fit_075_mae <- adapense_cv(x, y, alpha = 0.75, cv_k = 5, cv_repl = 5,                             cl = cluster, cv_type = \"naive\", cv_metric = mae)"},{"path":"/articles/computing_adapense.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Estimating predictive models","text":"D. Kepplinger S. Wei, “Information sharing robust stable cross-validation,” Technometrics, 2025, doi: 10.1080/00401706.2025.2540970. R. . Maronna, D. R. Martin, V. J. Yohai, M. Salibián-Barrera, Robust Statistics: Theory Methods (R). Wiley Series Probability Statistics. Hoboken, NJ: John Wiley & Sons, Inc., 2019.","code":""},{"path":"/articles/lambda_grids.html","id":"adjusting-the-grid-of-penalization-levels","dir":"Articles","previous_headings":"","what":"Adjusting the grid of penalization levels","title":"Controlling the grid of penalization levels","text":"default, adapense_cv() creates grid 50 penalization levels. grid spans “largest” penalization level (.e., penalization level solution intercept-model) multiple 10-3 largest penalization level. number grid points controlled argument nlambda=. grid points, models explored, computation time increases accordingly. applications “meaningful” change observed models differing least one predictor. Particularly lower end grid, estimated models often don’t differ number predictors exhibit marginal differences coefficient estimates. may waste computational resources exploring area little interest. can adjust lower endpoint automatic grid changing multiplier argument lambda_min_ratio=. ratio must less 1, closer 1 narrower grid. need re-focus grid larger values penalization level, can increase ratio, example, 0.5: model lowest penalization level still fairly sparse best model seems range, may need decrease ratio, e.g., 0.05: Prediction performance models estimated different grids penalization level: () narrow grid lambda_min_ratio=1e-1, (b) wide grid lambda_min_ratio=1e-6. plots can see grid left plot narrow, smaller penalization level 0.02 likely lead better predictive model. right, however, grid wide. Penalization levels less 0.01 seam substantially worse prediction accuracy. Therefore, can compute adaptive PENSE estimates better focused grid : Indeed, plot shows range interest (around minimum) covered several penalization levels: Prediction performance models estimated well-focused grid penalization levels.","code":"set.seed(1234) fit_grid_narrow <- adapense_cv(x, y, alpha = 0.75, lambda_min_ratio = 5e-1, cv_k = 5, cv_repl = 10) set.seed(1234) fit_grid_wide <- adapense_cv(x, y, alpha = 0.75, lambda_min_ratio = 5e-2, cv_k = 5, cv_repl = 10) set.seed(1234) fit_grid_focused <- adapense_cv(x, y, alpha = 0.75, lambda_min_ratio = 1e-1, cv_k = 5, cv_repl = 10)"},{"path":"/articles/lambda_grids.html","id":"manually-supplying-a-grid-of-penalization-levels","dir":"Articles","previous_headings":"Adjusting the grid of penalization levels","what":"Manually supplying a grid of penalization levels","title":"Controlling the grid of penalization levels","text":"function adapense_cv() friends also allows specify grid penalization levels via lambda= argument. Note adapense_cv() argument lambda= applies preliminary PENSE estimate adaptive PENSE estimate. cases, however, want. sensible grid penalization levels preliminary estimate general quite different good grid penalization levels adaptive estimate. compute adaptive PENSE estimates separately specified manual grids preliminary adaptive PENSE estimates, need compute manually via","code":"fit_preliminary <- pense_cv(x, y, alpha = 0, cv_k = 5, cv_repl = 10,                              lambda = exp(seq(log(0.5), log(20), length.out = 5))) exponent <- 1 penalty_loadings <- 1 / abs(coef(fit_preliminary)[-1])^exponent fit_adaptive <- pense_cv(x, y, alpha = 0.75, cv_k = 5, cv_repl = 10,                           lambda = exp(seq(log(0.1), log(2), length.out = 5))) summary(fit_adaptive) #> PENSE fit with prediction performance estimated by 10 replications of 5-fold  #> ris cross-validation. #>  #> 8 out of 40 predictors have non-zero coefficients: #>  #>                Estimate #> (Intercept)  1.56453280 #> X1           0.41947742 #> X2          -0.49314705 #> X3           0.41263669 #> X5           0.05041418 #> X26         -0.08230093 #> X29          0.03015557 #> X36         -0.07564768 #> X39         -0.03997095 #> --- #>  #> Hyper-parameters: lambda=0.9457416, alpha=0.75"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"David Kepplinger. Author, maintainer. Matías Salibián-Barrera. Author. Gabriela Cohen Freue. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kepplinger D, Salibián-Barrera M, Cohen Freue G (2026). pense: Penalized Elastic Net S/MM-Estimator Regression. R package version 2.5.2, https://dakep.github.io/pense-rpkg/.","code":"@Manual{,   title = {pense: Penalized Elastic Net S/MM-Estimator of Regression},   author = {David Kepplinger and Matías Salibián-Barrera and Gabriela {Cohen Freue}},   year = {2026},   note = {R package version 2.5.2},   url = {https://dakep.github.io/pense-rpkg/}, }"},{"path":"/index.html","id":"pense-r-package","dir":"","previous_headings":"","what":"Penalized Elastic Net S/MM-Estimator of Regression","title":"Penalized Elastic Net S/MM-Estimator of Regression","text":"R package implements penalized adaptive elastic net S-estimator (adaptive PENSE), non-adaptive version (PENSE) penalized M-step (PENSEM) proposed Cohen Freue, et al. (2019) Kepplinger (2023). Hyper-parameter selection done robust information sharing cross-validation (RIS-CV; Kepplinger & Wei 2025).","code":""},{"path":"/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Penalized Elastic Net S/MM-Estimator of Regression","text":"main function users adapense_cv(), computes adaptive PENSE fit estimates prediction performance many values penalization level. user can also compute non-adaptive PENSE fit using pense_cv() place adapense_cv() PENSEM fit using pensem_cv().","code":"library(pense)  # Generate dummy data with n=50 observations and p=25 possible predictors # (of which only the first 3 are truly relevant). n <- 50 p <- 25 set.seed(123) x <- matrix(rt(n * p, df = 5), ncol = p) y <- x[, 1] + 0.5 * x[, 2] + 2 * x[, 3] + rt(n, df = 2)  # Compute an adaptive PENSE fit with hyper-parameters alpha=0.8, exponent=2, # and 50 different values for the penalization level. # Prediction performance of the 50 fits for different penalization levels # is estimated with 5-fold cross-validation, # repeated 10 times (the more the better!). # On selected platforms use 2 CPU cores. set.seed(123) # Setting the seed is suggested for reproducibility of the CV results. fit <- adapense_cv(x, y, alpha = 0.9, cv_k = 5, cv_repl = 10, ncores = 2)  # Visualize the estimated prediction performance using plot() plot(fit)  # Summarize the model with best prediction performance using summary() summary(fit)  # Summarize the model with \"almost as-good prediction performance\" as the best # model using summary(lambda = \"se\") summary(fit, lambda = \"se\")"},{"path":"/index.html","id":"detailed-examples","dir":"","previous_headings":"","what":"Detailed examples","title":"Penalized Elastic Net S/MM-Estimator of Regression","text":"package vignette Estimating predictive models demonstrate detail compute adaptive non-adaptive PENSE estimates.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Penalized Elastic Net S/MM-Estimator of Regression","text":"install latest release CRAN, run following R code R console: recent development version can installed directly github using devtools package:","code":"install.packages(\"pense\") # Install the most recent development version: devtools::install_github(\"dakep/pense-rpkg\")"},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Penalized Elastic Net S/MM-Estimator of Regression","text":"D. Kepplinger S. Wei, “Information sharing robust stable cross-validation,” Technometrics, 2025, doi: 10.1080/00401706.2025.2540970. D. Kepplinger, “Robust variable selection estimation via adaptive elastic net S-estimators linear regression,” Computational Statistics & Data Analysis, vol. 183, 2023, doi: 10.1016/j.csda.2023.107730. G. V. Cohen Freue, D. Kepplinger, M. Salibián-Barrera, E. Smucler, “Robust elastic net estimators variable selection identification proteomic biomarkers,” Annals Applied Statistics, vol. 13, . 4, pp. 2065–2090, 2019, doi: 10.1214/19-AOAS1269.","code":""},{"path":"/reference/cd_algorithm_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinate Descent (CD) Algorithm to Compute Penalized Elastic Net S-estimates — cd_algorithm_options","title":"Coordinate Descent (CD) Algorithm to Compute Penalized Elastic Net S-estimates — cd_algorithm_options","text":"Set options CD algorithm compute adaptive EN S-estimates.","code":""},{"path":"/reference/cd_algorithm_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinate Descent (CD) Algorithm to Compute Penalized Elastic Net S-estimates — cd_algorithm_options","text":"","code":"cd_algorithm_options(   max_it = 1000,   reset_it = 8,   linesearch_steps = 4,   linesearch_mult = 0.5 )"},{"path":"/reference/cd_algorithm_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coordinate Descent (CD) Algorithm to Compute Penalized Elastic Net S-estimates — cd_algorithm_options","text":"max_it maximum number iterations. reset_it number iterations residuals re-computed scratch, prevent numerical drifts incremental updates. linesearch_steps maximum number steps used line search. linesearch_mult multiplier adjust step size line search.","code":""},{"path":"/reference/cd_algorithm_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coordinate Descent (CD) Algorithm to Compute Penalized Elastic Net S-estimates — cd_algorithm_options","text":"options CD algorithm compute (adaptive) PENSE estimates.","code":""},{"path":[]},{"path":"/reference/change_cv_measure.html","id":null,"dir":"Reference","previous_headings":"","what":"Change the Cross-Validation Measure — change_cv_measure","title":"Change the Cross-Validation Measure — change_cv_measure","text":"cross-validated fits using RIS-CV strategy, measure prediction accuracy can adjusted post-hoc.","code":""},{"path":"/reference/change_cv_measure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Change the Cross-Validation Measure — change_cv_measure","text":"","code":"change_cv_measure(   x,   measure = c(\"wrmspe\", \"wmape\", \"tau_size\", \"wrmspe_cv\", \"wmape_cv\"),   max_solutions = Inf )"},{"path":"/reference/change_cv_measure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Change the Cross-Validation Measure — change_cv_measure","text":"x fitted (adaptive) PENSE M-estimator measure measure use prediction accuracy max_solutions consider many best solutions. missing, solutions considered.","code":""},{"path":"/reference/change_cv_measure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Change the Cross-Validation Measure — change_cv_measure","text":"pense.cvfit object using updated measure prediction accuracy","code":""},{"path":[]},{"path":"/reference/coef.pense_cvfit.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficient Estimates — coef.pense_cvfit","title":"Extract Coefficient Estimates — coef.pense_cvfit","text":"Extract coefficients adaptive PENSE (LS-EN) regularization path hyper-parameters chosen cross-validation.","code":""},{"path":"/reference/coef.pense_cvfit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficient Estimates — coef.pense_cvfit","text":"","code":"# S3 method for class 'pense_cvfit' coef(   object,   alpha = NULL,   lambda = \"min\",   se_mult = 1,   sparse = NULL,   standardized = FALSE,   ... )"},{"path":"/reference/coef.pense_cvfit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficient Estimates — coef.pense_cvfit","text":"object PENSE cross-validated hyper-parameters extract coefficients . alpha Either single number NULL (default). given, fits given alpha value considered. lambda numeric value object fit multiple alpha values value provided, first value object$alpha used warning. lambda either string specifying penalty level use (\"min\", \"se\", \"{m}-se\") single numeric value penalty parameter. See details. se_mult lambda = \"se\", multiple standard errors tolerate. sparse coefficients returned sparse dense vectors? Defaults sparsity setting given object. Can also set sparse = 'matrix', case sparse matrix returned instead sparse vector. standardized return standardized coefficients. ... currently used.","code":""},{"path":"/reference/coef.pense_cvfit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficient Estimates — coef.pense_cvfit","text":"either numeric vector sparse vector type dsparseVector size \\(p + 1\\), depending sparse argument. Note: prior version 2.0.0 sparse coefficients returned sparse matrix type dgCMatrix. get sparse matrix previous versions, use sparse = 'matrix'.","code":""},{"path":"/reference/coef.pense_cvfit.html","id":"hyper-parameters","dir":"Reference","previous_headings":"","what":"Hyper-parameters","title":"Extract Coefficient Estimates — coef.pense_cvfit","text":"lambda = \"{m}-se\" object contains fitted estimates every penalization level sequence, use fit parsimonious model prediction performance statistically indistinguishable best model. determined model prediction performance within m * cv_se best model. lambda = \"se\", multiplier m taken se_mult. default alpha hyper-parameters available fitted object considered. can overridden supplying one multiple values parameter alpha. example, lambda = \"1-se\" alpha contains two values, \"1-SE\" rule applied individually alpha value, fit better prediction error considered. case lambda number object fit several alpha hyper-parameters, alpha must also given, first value object$alpha used warning.","code":""},{"path":[]},{"path":"/reference/coef.pense_cvfit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Coefficient Estimates — coef.pense_cvfit","text":"","code":"# Compute the PENSE regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- pense(x, freeny$y, alpha = 0.5) plot(regpath)   # Extract the coefficients at a certain penalization level coef(regpath, lambda = regpath$lambda[[1]][[40]]) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -6.5082299             0.2510560            -0.6879670  #>          income.level      market.potential  #>             0.7090986             0.9409940   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- pense_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 2, cv_k = 4) plot(cv_results, se_mult = 1)   # Print a summary of the fit and the cross-validation results. summary(cv_results) #> PENSE fit with prediction performance estimated by 2 replications of 4-fold ris  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -4.7921541 #> X1           0.3338834 #> X2          -0.6140406 #> X3           0.6954769 #> X4           0.7316339 #> --- #>  #> Hyper-parameters: lambda=0.0003364066, alpha=0.5  # Extract the coefficients at the penalization level with # smallest prediction error ... coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -4.7921541             0.3338834            -0.6140406  #>          income.level      market.potential  #>             0.6954769             0.7316339  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. coef(cv_results, lambda = '1-se') #>           (Intercept) lag.quarterly.revenue           price.index  #>           -11.4754472             0.2265866            -0.5739724  #>          income.level      market.potential  #>             0.5417608             1.3768215"},{"path":"/reference/coef.pense_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficient Estimates — coef.pense_fit","title":"Extract Coefficient Estimates — coef.pense_fit","text":"Extract coefficients adaptive PENSE (LS-EN) regularization path fitted pense() elnet().","code":""},{"path":"/reference/coef.pense_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficient Estimates — coef.pense_fit","text":"","code":"# S3 method for class 'pense_fit' coef(object, lambda, alpha = NULL, sparse = NULL, standardized = FALSE, ...)"},{"path":"/reference/coef.pense_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficient Estimates — coef.pense_fit","text":"object PENSE regularization path extract coefficients . lambda single number penalty level. alpha Either single number NULL (default). given, fits given alpha value considered. object fit multiple alpha values, value provided, first value object$alpha used warning. sparse coefficients returned sparse dense vectors? Defaults sparsity setting object. Can also set sparse = 'matrix', case sparse matrix returned instead sparse vector. standardized return standardized coefficients. ... currently used.","code":""},{"path":"/reference/coef.pense_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficient Estimates — coef.pense_fit","text":"either numeric vector sparse vector type dsparseVector size \\(p + 1\\), depending sparse argument. Note: prior version 2.0.0 sparse coefficients returned sparse matrix type dgCMatrix. get sparse matrix previous versions, use sparse = 'matrix'.","code":""},{"path":[]},{"path":"/reference/coef.pense_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Coefficient Estimates — coef.pense_fit","text":"","code":"# Compute the PENSE regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- pense(x, freeny$y, alpha = 0.5) plot(regpath)   # Extract the coefficients at a certain penalization level coef(regpath, lambda = regpath$lambda[[1]][[40]]) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -6.5082299             0.2510560            -0.6879670  #>          income.level      market.potential  #>             0.7090986             0.9409940   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- pense_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 2, cv_k = 4) plot(cv_results, se_mult = 1)   # Print a summary of the fit and the cross-validation results. summary(cv_results) #> PENSE fit with prediction performance estimated by 2 replications of 4-fold ris  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -4.7921541 #> X1           0.3338834 #> X2          -0.6140406 #> X3           0.6954769 #> X4           0.7316339 #> --- #>  #> Hyper-parameters: lambda=0.0003364066, alpha=0.5  # Extract the coefficients at the penalization level with # smallest prediction error ... coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -4.7921541             0.3338834            -0.6140406  #>          income.level      market.potential  #>             0.6954769             0.7316339  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. coef(cv_results, lambda = '1-se') #>           (Intercept) lag.quarterly.revenue           price.index  #>           -11.4754472             0.2265866            -0.5739724  #>          income.level      market.potential  #>             0.5417608             1.3768215"},{"path":"/reference/dot-approx_match.html","id":null,"dir":"Reference","previous_headings":"","what":"Approximate Value Matching — .approx_match","title":"Approximate Value Matching — .approx_match","text":"Approximate Value Matching","code":""},{"path":"/reference/dot-approx_match.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approximate Value Matching — .approx_match","text":"","code":".approx_match(x, table, eps)"},{"path":"/reference/dot-approx_match.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approximate Value Matching — .approx_match","text":"x, table see base::match details. eps numerical tolerance matching.","code":""},{"path":"/reference/dot-approx_match.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Approximate Value Matching — .approx_match","text":"vector length x integers giving position table first match match, NA_integer_ otherwise.","code":""},{"path":"/reference/dot-bisquare_consistency_const.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function — .bisquare_consistency_const","title":"Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function — .bisquare_consistency_const","text":"Get Constant Consistency M-Scale Using Bisquare Rho Function","code":""},{"path":"/reference/dot-bisquare_consistency_const.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function — .bisquare_consistency_const","text":"","code":".bisquare_consistency_const(delta, eps = sqrt(.Machine$double.eps))"},{"path":"/reference/dot-bisquare_consistency_const.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function — .bisquare_consistency_const","text":"delta desired breakdown point (0 0.5) eps numerical tolerance equality comparisons","code":""},{"path":"/reference/dot-bisquare_consistency_const.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function — .bisquare_consistency_const","text":"consistency constant","code":""},{"path":"/reference/dot-bisquare_efficiency_const.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the constant for the desired efficiency of the M-estimate of location using the bisquare \\(\\rho\\) function — .bisquare_efficiency_const","title":"Get the constant for the desired efficiency of the M-estimate of location using the bisquare \\(\\rho\\) function — .bisquare_efficiency_const","text":"Get constant desired efficiency M-estimate location using bisquare \\(\\rho\\) function","code":""},{"path":"/reference/dot-bisquare_efficiency_const.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the constant for the desired efficiency of the M-estimate of location using the bisquare \\(\\rho\\) function — .bisquare_efficiency_const","text":"","code":".bisquare_efficiency_const(eff, eps = sqrt(.Machine$double.eps))"},{"path":"/reference/dot-bisquare_efficiency_const.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the constant for the desired efficiency of the M-estimate of location using the bisquare \\(\\rho\\) function — .bisquare_efficiency_const","text":"eff desired efficiency (0 1) eps numerical tolerance equality comparisons","code":""},{"path":"/reference/dot-bisquare_efficiency_const.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the constant for the desired efficiency of the M-estimate of location using the bisquare \\(\\rho\\) function — .bisquare_efficiency_const","text":"tuning constant desired efficiency","code":""},{"path":"/reference/dot-find_stable_bdb_bisquare.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine a breakdown point with stable numerical properties of the M-scale with Tukey's bisquare rho function. — .find_stable_bdb_bisquare","title":"Determine a breakdown point with stable numerical properties of the M-scale with Tukey's bisquare rho function. — .find_stable_bdb_bisquare","text":"M-scale objective (hence S-loss) can unbounded high 1st derivative. can lead numerical instability algorithms turn excessive computation time. function chooses breakdown point lowest upper bound 1st derivative range bdp's vicinity desired bdp.","code":""},{"path":"/reference/dot-find_stable_bdb_bisquare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine a breakdown point with stable numerical properties of the M-scale with Tukey's bisquare rho function. — .find_stable_bdb_bisquare","text":"","code":".find_stable_bdb_bisquare(   n,   desired_bdp,   tolerance = 0.01,   precision = 1e-04,   interval = c(0.05, 0.5),   eps = sqrt(.Machine$double.eps) )"},{"path":"/reference/dot-find_stable_bdb_bisquare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine a breakdown point with stable numerical properties of the M-scale with Tukey's bisquare rho function. — .find_stable_bdb_bisquare","text":"n number observations sample desired_bdp desired breakdown point (0.05 0.5) tolerance far can chosen bdp away desired bdp. chosen bdp guaranteed range given interval. precision granularity grid considered bdp's. interval restrict chosen bdp interval. eps numerical tolerance equality comparisons","code":""},{"path":"/reference/dot-huber_efficiency_const.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the constant for the desired efficiency of the M-estimate of location using the Huber \\(\\rho\\) function — .huber_efficiency_const","title":"Get the constant for the desired efficiency of the M-estimate of location using the Huber \\(\\rho\\) function — .huber_efficiency_const","text":"Get constant desired efficiency M-estimate location using Huber \\(\\rho\\) function","code":""},{"path":"/reference/dot-huber_efficiency_const.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the constant for the desired efficiency of the M-estimate of location using the Huber \\(\\rho\\) function — .huber_efficiency_const","text":"","code":".huber_efficiency_const(eff, eps = sqrt(.Machine$double.eps))"},{"path":"/reference/dot-huber_efficiency_const.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the constant for the desired efficiency of the M-estimate of location using the Huber \\(\\rho\\) function — .huber_efficiency_const","text":"eff desired efficiency (0 1) eps numerical tolerance equality comparisons","code":""},{"path":"/reference/dot-huber_efficiency_const.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the constant for the desired efficiency of the M-estimate of location using the Huber \\(\\rho\\) function — .huber_efficiency_const","text":"tuning constant desired efficiency","code":""},{"path":"/reference/dot-mopt_consistency_const.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Constant for Consistency for the M-Scale Using the Optimal Rho Function — .mopt_consistency_const","title":"Get the Constant for Consistency for the M-Scale Using the Optimal Rho Function — .mopt_consistency_const","text":"Get Constant Consistency M-Scale Using Optimal Rho Function","code":""},{"path":"/reference/dot-mopt_consistency_const.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Constant for Consistency for the M-Scale Using the Optimal Rho Function — .mopt_consistency_const","text":"","code":".mopt_consistency_const(delta, eps = sqrt(.Machine$double.eps))"},{"path":"/reference/dot-mopt_consistency_const.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Constant for Consistency for the M-Scale Using the Optimal Rho Function — .mopt_consistency_const","text":"delta desired breakdown point (0 0.5) eps numerical tolerance equality comparisons","code":""},{"path":"/reference/dot-mopt_consistency_const.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Constant for Consistency for the M-Scale Using the Optimal Rho Function — .mopt_consistency_const","text":"consistency constant","code":""},{"path":"/reference/dot-mopt_efficiency_const.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the constant for the desired efficiency of the M-estimate of location using the optimal \\(\\rho\\) function — .mopt_efficiency_const","title":"Get the constant for the desired efficiency of the M-estimate of location using the optimal \\(\\rho\\) function — .mopt_efficiency_const","text":"Get constant desired efficiency M-estimate location using optimal \\(\\rho\\) function","code":""},{"path":"/reference/dot-mopt_efficiency_const.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the constant for the desired efficiency of the M-estimate of location using the optimal \\(\\rho\\) function — .mopt_efficiency_const","text":"","code":".mopt_efficiency_const(eff, eps = sqrt(.Machine$double.eps))"},{"path":"/reference/dot-mopt_efficiency_const.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the constant for the desired efficiency of the M-estimate of location using the optimal \\(\\rho\\) function — .mopt_efficiency_const","text":"eff desired efficiency (0 1) eps numerical tolerance equality comparisons","code":""},{"path":"/reference/dot-mopt_efficiency_const.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the constant for the desired efficiency of the M-estimate of location using the optimal \\(\\rho\\) function — .mopt_efficiency_const","text":"tuning constant desired efficiency","code":""},{"path":"/reference/dot-run_replicated_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Run replicated K-fold CV with random splits — .run_replicated_cv","title":"Run replicated K-fold CV with random splits — .run_replicated_cv","text":"Run replicated K-fold CV random splits","code":""},{"path":"/reference/dot-run_replicated_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run replicated K-fold CV with random splits — .run_replicated_cv","text":"","code":".run_replicated_cv(   std_data,   cv_k,   cv_repl,   cv_est_fun,   metric,   par_cluster = NULL,   handler_args = list() )"},{"path":"/reference/dot-run_replicated_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run replicated K-fold CV with random splits — .run_replicated_cv","text":"std_data standardized full data set (standardized .standardize_data) cv_k number folds per CV split cv_repl number CV replications. cv_est_fun function taking standardized training set indices left-observations returns list estimates. function always needs return number estimates! metric function taking vector prediction errors returning scale prediction error. par_cluster parallel cluster parallelize computations. handler_args additional arguments handler function.","code":""},{"path":"/reference/dot-run_replicated_cv_ris.html","id":null,"dir":"Reference","previous_headings":"","what":"Run replicated K-fold CV with random splits, matching the global estimates to the CV estimates by Kendall's tau-b computed on the robustness weights. — .run_replicated_cv_ris","title":"Run replicated K-fold CV with random splits, matching the global estimates to the CV estimates by Kendall's tau-b computed on the robustness weights. — .run_replicated_cv_ris","text":"Run replicated K-fold CV random splits, matching global estimates CV estimates Kendall's tau-b computed robustness weights.","code":""},{"path":"/reference/dot-run_replicated_cv_ris.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run replicated K-fold CV with random splits, matching the global estimates to the CV estimates by Kendall's tau-b computed on the robustness weights. — .run_replicated_cv_ris","text":"","code":".run_replicated_cv_ris(   std_data,   cv_k,   cv_repl,   cv_est_fun,   global_ests,   min_similarity = 0,   par_cluster = NULL,   rho_opts,   handler_args = list() )"},{"path":"/reference/dot-run_replicated_cv_ris.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run replicated K-fold CV with random splits, matching the global estimates to the CV estimates by Kendall's tau-b computed on the robustness weights. — .run_replicated_cv_ris","text":"std_data standardized full data set (standardized .standardize_data) cv_k number folds per CV split cv_repl number CV replications. cv_est_fun function taking standardized training set indices left-observations returns list estimates. function always needs return number estimates! global_ests estimates computed observations. min_similarity minimum (average) similarity CV solutions considered (0 1). CV solution satisfies lower bound, best CV solution used regardless similarity. par_cluster parallel cluster parallelize computations. rho_opts rho function options. handler_args additional arguments handler function.","code":""},{"path":"/reference/dot-standardize_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Standardize data — .standardize_data","title":"Standardize data — .standardize_data","text":"Standardize data","code":""},{"path":"/reference/dot-standardize_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standardize data — .standardize_data","text":"","code":".standardize_data(   x,   y,   intercept,   standardize,   robust,   sparse,   mscale_opts,   cc,   target_scale_x = NULL,   ... )"},{"path":"/reference/dot-standardize_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standardize data — .standardize_data","text":"x predictor matrix. Can also list components x y, case y ignored. y response vector. intercept intercept included (.e., y centered?) standardize standardize . robust use robust standardization. cc cutoff value rho functions used scale location estimates. ... passed mlocscale().","code":""},{"path":"/reference/dot-standardize_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standardize data — .standardize_data","text":"list following entries:","code":""},{"path":"/reference/elnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the Least Squares (Adaptive) Elastic Net Regularization Path — elnet","title":"Compute the Least Squares (Adaptive) Elastic Net Regularization Path — elnet","text":"Compute least squares EN estimates linear regression optional observation weights penalty loadings.","code":""},{"path":"/reference/elnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the Least Squares (Adaptive) Elastic Net Regularization Path — elnet","text":"","code":"elnet(   x,   y,   alpha,   nlambda = 100,   lambda_min_ratio,   lambda,   penalty_loadings,   weights,   intercept = TRUE,   en_algorithm_opts,   sparse = FALSE,   eps = 1e-06,   standardize = TRUE )"},{"path":"/reference/elnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the Least Squares (Adaptive) Elastic Net Regularization Path — elnet","text":"x n p matrix numeric predictors. y vector response values length n. binary classification, y factor 2 levels. alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. Can vector several values, alpha = 0 mixed values. nlambda number penalization levels. lambda_min_ratio Smallest value penalization level fraction largest level (.e., smallest value coefficients zero). default depends sample size relative number variables alpha. observations variables available, default 1e-3 * alpha, otherwise 1e-2 * alpha. lambda optional user-supplied sequence penalization levels. given NULL, nlambda lambda_min_ratio ignored. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. weights vector positive observation weights. intercept include intercept model. en_algorithm_opts options EN algorithm. See en_algorithm_options details. sparse use sparse coefficient vectors. eps numerical tolerance. standardize standardize variables unit variance. Coefficients always returned original scale.","code":""},{"path":"/reference/elnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the Least Squares (Adaptive) Elastic Net Regularization Path — elnet","text":"list-like object following items alpha sequence alpha parameters. lambda list sequences penalization levels, one per alpha parameter. estimates list estimates. estimate contains following information: intercept intercept estimate. beta beta (slope) estimate. lambda penalization level estimate computed. alpha alpha hyper-parameter estimate computed. statuscode > 0 algorithm experienced issues computing estimate. status optional status message algorithm. call original call.","code":""},{"path":"/reference/elnet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the Least Squares (Adaptive) Elastic Net Regularization Path — elnet","text":"elastic net estimator linear regression model solves optimization problem $$argmin_{\\mu, \\beta}   (1/2n) \\sum_i w_i (y_i - \\mu - x_i' \\beta)^2 +   \\lambda \\sum_j 0.5 (1 - \\alpha) \\beta_j^2 + \\alpha l_j |\\beta_j|  $$ observation weights \\(w_i\\) penalty loadings \\(l_j\\).","code":""},{"path":[]},{"path":"/reference/elnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the Least Squares (Adaptive) Elastic Net Regularization Path — elnet","text":"","code":"# Compute the LS-EN regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- elnet(x, freeny$y, alpha = c(0.5, 0.75)) plot(regpath)  plot(regpath, alpha = 0.75)   # Extract the coefficients at a certain penalization level coef(regpath, lambda = regpath$lambda[[1]][[5]],      alpha = 0.75) #>           (Intercept) lag.quarterly.revenue           price.index  #>              9.306304              0.000000              0.000000  #>          income.level      market.potential  #>              0.000000              0.000000   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- elnet_cv(x, freeny$y, alpha = c(0.5, 0.75),                        cv_repl = 10, cv_k = 4,                        cv_measure = \"tau\") plot(cv_results, se_mult = 1.5)  plot(cv_results, se_mult = 1.5, what = \"coef.path\")    # Extract the coefficients at the penalization level with # smallest prediction error ... summary(cv_results) #> EN fit with prediction performance estimated by replications of 4-fold  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -9.6491805 #> X1           0.1899399 #> X2          -0.6858733 #> X3           0.7075924 #> X4           1.2247539 #> --- #>  #> Hyper-parameters: lambda=0.003787891, alpha=0.5 coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.6491805             0.1899399            -0.6858733  #>          income.level      market.potential  #>             0.7075924             1.2247539  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. summary(cv_results, lambda = \"1.5-se\") #> EN fit with prediction performance estimated by replications of 4-fold  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -9.5875726 #> X1           0.2270959 #> X2          -0.6216322 #> X3           0.6519060 #> X4           1.1972788 #> --- #>  #> Hyper-parameters: lambda=0.01303176, alpha=0.5 coef(cv_results, lambda = \"1.5-se\") #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.5875726             0.2270959            -0.6216322  #>          income.level      market.potential  #>             0.6519060             1.1972788"},{"path":"/reference/elnet_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates — elnet_cv","title":"Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates — elnet_cv","text":"Perform (repeated) K-fold cross-validation elnet().","code":""},{"path":"/reference/elnet_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates — elnet_cv","text":"","code":"elnet_cv(   x,   y,   lambda,   cv_k,   cv_repl = 1,   cv_type = \"naive\",   cv_metric = c(\"rmspe\", \"tau_size\", \"mape\", \"auroc\"),   fit_all = TRUE,   cl = NULL,   ... )"},{"path":"/reference/elnet_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates — elnet_cv","text":"x n p matrix numeric predictors. y vector response values length n. binary classification, y factor 2 levels. lambda optional user-supplied sequence penalization levels. given NULL, nlambda lambda_min_ratio ignored. cv_k number folds per cross-validation. cv_repl number cross-validation replications. cv_type kind cross-validation performed: robust information sharing (ris) standard (naive) CV. cv_metric cv_type='naive'. Either string specifying performance metric use, function evaluate prediction errors single CV replication. function, number arguments define data function receives. function takes single argument, called single numeric vector prediction errors. function takes two arguments, called predicted values first argument true values second argument. function must always return single numeric value quantifying prediction performance. order given values corresponds order input data. fit_all cv_type='naive'. TRUE, fit model penalization levels. Can also combination \"min\" \"{x}-se\", case models penalization level smallest average CV accuracy, within {x} standard errors, respectively. Setting fit_all FALSE equivalent \"min\". Applies alpha value. cl parallel cluster. Can used combination ncores = 1. ... Arguments passed elnet alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. Can vector several values, alpha = 0 mixed values. nlambda number penalization levels. lambda_min_ratio Smallest value penalization level fraction largest level (.e., smallest value coefficients zero). default depends sample size relative number variables alpha. observations variables available, default 1e-3 * alpha, otherwise 1e-2 * alpha. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. standardize standardize variables unit variance. Coefficients always returned original scale. weights vector positive observation weights. intercept include intercept model. sparse use sparse coefficient vectors. en_algorithm_opts options EN algorithm. See en_algorithm_options details. eps numerical tolerance.","code":""},{"path":"/reference/elnet_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates — elnet_cv","text":"list-like object components returned elnet(), plus following: cvres data frame average cross-validated performance.","code":""},{"path":"/reference/elnet_cv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates — elnet_cv","text":"built-CV metrics \"tau_size\" \\(\\tau\\)-size prediction error, computed tau_size() (default). \"mape\" Median absolute prediction error. \"rmspe\" Root mean squared prediction error. \"auroc\" Area receiver operator characteristic curve (actually 1 - AUROC). sensible binary responses.","code":""},{"path":[]},{"path":"/reference/elnet_cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates — elnet_cv","text":"","code":"# Compute the LS-EN regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- elnet(x, freeny$y, alpha = c(0.5, 0.75)) plot(regpath)  plot(regpath, alpha = 0.75)   # Extract the coefficients at a certain penalization level coef(regpath, lambda = regpath$lambda[[1]][[5]],      alpha = 0.75) #>           (Intercept) lag.quarterly.revenue           price.index  #>              9.306304              0.000000              0.000000  #>          income.level      market.potential  #>              0.000000              0.000000   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- elnet_cv(x, freeny$y, alpha = c(0.5, 0.75),                        cv_repl = 10, cv_k = 4,                        cv_measure = \"tau\") plot(cv_results, se_mult = 1.5)  plot(cv_results, se_mult = 1.5, what = \"coef.path\")    # Extract the coefficients at the penalization level with # smallest prediction error ... summary(cv_results) #> EN fit with prediction performance estimated by replications of 4-fold  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -9.6491805 #> X1           0.1899399 #> X2          -0.6858733 #> X3           0.7075924 #> X4           1.2247539 #> --- #>  #> Hyper-parameters: lambda=0.003787891, alpha=0.5 coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.6491805             0.1899399            -0.6858733  #>          income.level      market.potential  #>             0.7075924             1.2247539  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. summary(cv_results, lambda = \"1.5-se\") #> EN fit with prediction performance estimated by replications of 4-fold  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -9.5875726 #> X1           0.2270959 #> X2          -0.6216322 #> X3           0.6519060 #> X4           1.1972788 #> --- #>  #> Hyper-parameters: lambda=0.01303176, alpha=0.5 coef(cv_results, lambda = \"1.5-se\") #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.5875726             0.2270959            -0.6216322  #>          income.level      market.potential  #>             0.6519060             1.1972788"},{"path":"/reference/en_admm_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Use the ADMM Elastic Net Algorithm — en_admm_options","title":"Use the ADMM Elastic Net Algorithm — en_admm_options","text":"Use ADMM Elastic Net Algorithm","code":""},{"path":"/reference/en_admm_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use the ADMM Elastic Net Algorithm — en_admm_options","text":"","code":"en_admm_options(max_it = 1000, step_size, acceleration = 1)"},{"path":"/reference/en_admm_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use the ADMM Elastic Net Algorithm — en_admm_options","text":"max_it maximum number iterations. step_size step size algorithm. acceleration acceleration factor linearized ADMM.","code":""},{"path":"/reference/en_admm_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use the ADMM Elastic Net Algorithm — en_admm_options","text":"options ADMM EN algorithm.","code":""},{"path":[]},{"path":"/reference/en_algorithm_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Control the Algorithm to Compute (Weighted) Least-Squares Elastic Net Estimates — en_algorithm_options","title":"Control the Algorithm to Compute (Weighted) Least-Squares Elastic Net Estimates — en_algorithm_options","text":"package supports different algorithms compute EN estimate weighted LS loss functions. algorithm certain characteristics make useful problems. select specific algorithm adjust options, use en_***_options functions.","code":""},{"path":"/reference/en_algorithm_options.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Control the Algorithm to Compute (Weighted) Least-Squares Elastic Net Estimates — en_algorithm_options","text":"en_lars_options(): Use tuning-free LARS algorithm. computes exact (numerical errors) solutions EN-LS problem. iterative therefore can benefit approximate solutions, turn guarantees solution found. en_cd_options(): Use iterative coordinate descent algorithm needs \\(O(n p)\\) operations per iteration converges sub-linearly. en_admm_options(): Use iterative ADMM-type algorithm needs \\(O(n p)\\) operations per iteration converges sub-linearly. en_dal_options(): Use iterative Dual Augmented Lagrangian (DAL) method. DAL needs \\(O(n^3 p^2)\\) operations per iteration, converges exponentially.","code":""},{"path":[]},{"path":"/reference/en_cd_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Use Coordinate Descent to Solve Elastic Net Problems — en_cd_options","title":"Use Coordinate Descent to Solve Elastic Net Problems — en_cd_options","text":"Use Coordinate Descent Solve Elastic Net Problems","code":""},{"path":"/reference/en_cd_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use Coordinate Descent to Solve Elastic Net Problems — en_cd_options","text":"","code":"en_cd_options(max_it = 1000, reset_it = 8)"},{"path":"/reference/en_cd_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use Coordinate Descent to Solve Elastic Net Problems — en_cd_options","text":"max_it maximum number iterations. reset_it number iterations residuals re-computed scratch, prevent numerical drifts incremental updates.","code":""},{"path":[]},{"path":"/reference/en_dal_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Use the DAL Elastic Net Algorithm — en_dal_options","title":"Use the DAL Elastic Net Algorithm — en_dal_options","text":"Use DAL Elastic Net Algorithm","code":""},{"path":"/reference/en_dal_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use the DAL Elastic Net Algorithm — en_dal_options","text":"","code":"en_dal_options(   max_it = 100,   max_inner_it = 100,   eta_multiplier = 2,   eta_start_conservative = 0.01,   eta_start_aggressive = 1,   lambda_relchange_aggressive = 0.25 )"},{"path":"/reference/en_dal_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use the DAL Elastic Net Algorithm — en_dal_options","text":"max_it maximum number (outer) iterations. max_inner_it maximum number (inner) iterations outer iteration. eta_multiplier multiplier barrier parameter. iteration, barrier must restrictive (.e., multiplier must > 1). eta_start_conservative conservative initial barrier parameter. used previous penalty undefined far away. eta_start_aggressive aggressive initial barrier parameter. used previous penalty close. lambda_relchange_aggressive close must lambda parameter previous penalty term use aggressive initial barrier parameter (.e., constitutes \"far\").","code":""},{"path":"/reference/en_dal_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use the DAL Elastic Net Algorithm — en_dal_options","text":"options DAL EN algorithm.","code":""},{"path":[]},{"path":"/reference/en_lars_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Use the LARS Elastic Net Algorithm — en_lars_options","title":"Use the LARS Elastic Net Algorithm — en_lars_options","text":"Use LARS Elastic Net Algorithm","code":""},{"path":"/reference/en_lars_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use the LARS Elastic Net Algorithm — en_lars_options","text":"","code":"en_lars_options()"},{"path":[]},{"path":"/reference/en_ridge_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Ridge optimizer using an Augmented data matrix. Only available for Ridge problems (`alpha=0“) and selected automatically in this case. — en_ridge_options","title":"Ridge optimizer using an Augmented data matrix. Only available for Ridge problems (`alpha=0“) and selected automatically in this case. — en_ridge_options","text":"Ridge optimizer using Augmented data matrix. available Ridge problems (`alpha=0“) selected automatically case.","code":""},{"path":"/reference/en_ridge_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ridge optimizer using an Augmented data matrix. Only available for Ridge problems (`alpha=0“) and selected automatically in this case. — en_ridge_options","text":"","code":"en_ridge_options()"},{"path":"/reference/enpy_initial_estimates.html","id":null,"dir":"Reference","previous_headings":"","what":"ENPY Initial Estimates for EN S-Estimators — enpy_initial_estimates","title":"ENPY Initial Estimates for EN S-Estimators — enpy_initial_estimates","text":"Compute initial estimates EN S-estimator using EN-PY procedure.","code":""},{"path":"/reference/enpy_initial_estimates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ENPY Initial Estimates for EN S-Estimators — enpy_initial_estimates","text":"","code":"enpy_initial_estimates(   x,   y,   alpha,   lambda,   bdp = 0.25,   cc,   intercept = TRUE,   penalty_loadings,   enpy_opts = enpy_options(),   mscale_opts = mscale_algorithm_options(),   eps = 1e-06,   sparse = FALSE,   ncores = 1L )"},{"path":"/reference/enpy_initial_estimates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ENPY Initial Estimates for EN S-Estimators — enpy_initial_estimates","text":"x n p matrix numeric predictors. y vector response values length n. alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. Can vector several values, alpha = 0 mixed values. lambda vector positive values penalization levels. bdp desired breakdown point estimator, 0.05 0.5. actual breakdown point may slightly larger/smaller avoid instabilities S-loss. cc cutoff value rho function. default, chosen yield consistent estimate Normal distribution. intercept include intercept model. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. allowed alpha > 0. enpy_opts options EN-PY algorithm, created enpy_options() function. mscale_opts options M-scale estimation. See mscale_algorithm_options() details. eps numerical tolerance. sparse use sparse coefficient vectors. ncores number CPU cores use parallel. default, one CPU core used. supported platforms, case warning given.","code":""},{"path":"/reference/enpy_initial_estimates.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ENPY Initial Estimates for EN S-Estimators — enpy_initial_estimates","text":"manually computed initial estimates intended starting points pense(), default shared penalization levels. restrict use initial estimates penalty level computed , use as_starting_point(..., specific = TRUE). See as_starting_point() details.","code":""},{"path":"/reference/enpy_initial_estimates.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ENPY Initial Estimates for EN S-Estimators — enpy_initial_estimates","text":"Cohen Freue, G.V.; Kepplinger, D.; Salibián-Barrera, M.; Smucler, E. Robust elastic net estimators variable selection identification proteomic biomarkers. Ann. Appl. Stat. 13 (2019), . 4, 2065–2090 doi:10.1214/19-AOAS1269","code":""},{"path":[]},{"path":"/reference/enpy_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Options for the ENPY Algorithm — enpy_options","title":"Options for the ENPY Algorithm — enpy_options","text":"Additional control options elastic net Peña-Yohai procedure.","code":""},{"path":"/reference/enpy_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Options for the ENPY Algorithm — enpy_options","text":"","code":"enpy_options(   max_it = 10,   keep_psc_proportion = 0.5,   en_algorithm_opts,   keep_residuals_measure = c(\"threshold\", \"proportion\"),   keep_residuals_proportion = 0.5,   keep_residuals_threshold = 2,   retain_best_factor = 2,   retain_max = 500 )"},{"path":"/reference/enpy_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Options for the ENPY Algorithm — enpy_options","text":"max_it maximum number EN-PY iterations. keep_psc_proportion many observations keep based Principal Sensitivity Components. en_algorithm_opts options LS-EN algorithm. See en_algorithm_options details. keep_residuals_measure determine observations keep, based residuals. proportion, fixed number observations kept. threshold, observations residuals threshold kept. keep_residuals_proportion proportion observations kept based residuals. keep_residuals_threshold observations (standardized) residuals less threshold kept. retain_best_factor keep candidates within factor best candidate. <= 1, keep candidates last iteration. retain_max maximum number candidates, .e., best retain_max candidates retained.","code":""},{"path":"/reference/enpy_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Options for the ENPY Algorithm — enpy_options","text":"options ENPY algorithm.","code":""},{"path":"/reference/enpy_options.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Options for the ENPY Algorithm — enpy_options","text":"EN-PY procedure computing initial estimates iteratively cleans data observations possibly outlying residual high leverage. Least-squares elastic net (LS-EN) estimates computed possibly clean subsets. iteration, Principal Sensitivity Components computed remove observations potentially high leverage. Among LS-EN estimates, estimate smallest M-scale residuals selected. Observations largest residual selected estimate removed next iteration started.","code":""},{"path":[]},{"path":"/reference/mloc.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the M-estimate of Location — mloc","title":"Compute the M-estimate of Location — mloc","text":"Compute M-estimate location using auxiliary estimate scale.","code":""},{"path":"/reference/mloc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the M-estimate of Location — mloc","text":"","code":"mloc(x, scale, rho = \"bisquare\", eff = 0.9, cc, max_it = 200, eps = 1e-08)"},{"path":"/reference/mloc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the M-estimate of Location — mloc","text":"x numeric values. Missing values verbosely ignored. scale scale x values. omitted, uses mad(). rho \\(\\rho\\) function use. See rho_function() available functions. eff desired efficiency Normal model. cc value tuning constant chosen \\(\\rho\\) function. specified, overrides desired efficiency. max_it maximum number iterations. eps numerical tolerance check convergence.","code":""},{"path":"/reference/mloc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the M-estimate of Location — mloc","text":"single numeric value, M-estimate location.","code":""},{"path":[]},{"path":"/reference/mlocscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the M-estimate of Location and Scale — mlocscale","title":"Compute the M-estimate of Location and Scale — mlocscale","text":"Simultaneous estimation location scale means M-estimates.","code":""},{"path":"/reference/mlocscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the M-estimate of Location and Scale — mlocscale","text":"","code":"mlocscale(   x,   bdp = 0.25,   eff = 0.9,   scale_cc,   location_rho,   location_cc,   opts = mscale_algorithm_options() )"},{"path":"/reference/mlocscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the M-estimate of Location and Scale — mlocscale","text":"x numeric values. Missing values verbosely ignored. bdp desired breakdown point (0 0.5). eff desired efficiency location estimate (0.1 0.99). scale_cc tuning constant \\(\\rho\\) function computing scale estimate. default, chosen yield consistent estimate normally distributed values. location_rho \\(\\rho\\) function computing location estimate. missing, use function scale estimate (opts$rho). See rho_function() list available \\(\\rho\\) functions. location_cc tuning constant location \\(\\rho\\) function. default chosen yield desired efficiency. provided, desired efficiency ignored. opts list options M-scale estimating equations, See mscale_algorithm_options() details.","code":""},{"path":"/reference/mlocscale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the M-estimate of Location and Scale — mlocscale","text":"vector 2 elements, M-estimate location M-scale estimate.","code":""},{"path":[]},{"path":"/reference/mm_algorithm_options.html","id":null,"dir":"Reference","previous_headings":"","what":"MM-Algorithm to Compute Penalized Elastic Net S- and M-Estimates — mm_algorithm_options","title":"MM-Algorithm to Compute Penalized Elastic Net S- and M-Estimates — mm_algorithm_options","text":"Additional options MM algorithm compute EN S- M-estimates.","code":""},{"path":"/reference/mm_algorithm_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MM-Algorithm to Compute Penalized Elastic Net S- and M-Estimates — mm_algorithm_options","text":"","code":"mm_algorithm_options(   max_it = 500,   tightening = c(\"adaptive\", \"exponential\", \"none\"),   tightening_steps = 2,   en_algorithm_opts )"},{"path":"/reference/mm_algorithm_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MM-Algorithm to Compute Penalized Elastic Net S- and M-Estimates — mm_algorithm_options","text":"max_it maximum number iterations. tightening make inner iterations precise algorithm approaches local minimum. tightening_steps adaptive tightening strategy, often tighten desired tolerance attained. en_algorithm_opts options inner LS-EN algorithm. See en_algorithm_options details.","code":""},{"path":"/reference/mm_algorithm_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MM-Algorithm to Compute Penalized Elastic Net S- and M-Estimates — mm_algorithm_options","text":"options MM algorithm.","code":""},{"path":[]},{"path":"/reference/mscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the M-Scale of Centered Values — mscale","title":"Compute the M-Scale of Centered Values — mscale","text":"Compute M-scale without centering values.","code":""},{"path":"/reference/mscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the M-Scale of Centered Values — mscale","text":"","code":"mscale(x, bdp = 0.25, cc, opts = mscale_algorithm_options())"},{"path":"/reference/mscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the M-Scale of Centered Values — mscale","text":"x numeric values. Missing values verbosely ignored. bdp desired breakdown point (0 0.5). cc tuning parameters chosen rho function. default, chosen yield consistent estimate Normal distribution. opts list options M-scale estimation algorithm, see mscale_algorithm_options() details.","code":""},{"path":"/reference/mscale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the M-Scale of Centered Values — mscale","text":"M-estimate scale.","code":""},{"path":[]},{"path":"/reference/mscale_algorithm_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Options for the M-scale Estimation Algorithm — mscale_algorithm_options","title":"Options for the M-scale Estimation Algorithm — mscale_algorithm_options","text":"Options M-scale Estimation Algorithm","code":""},{"path":"/reference/mscale_algorithm_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Options for the M-scale Estimation Algorithm — mscale_algorithm_options","text":"","code":"mscale_algorithm_options(rho = \"bisquare\", max_it = 200, eps = 1e-08)"},{"path":"/reference/mscale_algorithm_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Options for the M-scale Estimation Algorithm — mscale_algorithm_options","text":"rho \\(\\rho\\) function use. See rho_function() possible values. max_it maximum number iterations. eps numerical tolerance check convergence.","code":""},{"path":"/reference/mscale_algorithm_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Options for the M-scale Estimation Algorithm — mscale_algorithm_options","text":"options M-scale estimation algorithm.","code":""},{"path":[]},{"path":"/reference/mscale_derivative.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the Gradient and Hessian of the M-Scale Function — mscale_derivative","title":"Compute the Gradient and Hessian of the M-Scale Function — mscale_derivative","text":"Compute derivative (gradient) Hessian M-scale function evaluated point x.","code":""},{"path":"/reference/mscale_derivative.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the Gradient and Hessian of the M-Scale Function — mscale_derivative","text":"","code":"mscale_derivative(   x,   bdp = 0.25,   order = 1,   cc,   opts = mscale_algorithm_options() )"},{"path":"/reference/mscale_derivative.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the Gradient and Hessian of the M-Scale Function — mscale_derivative","text":"x numeric values. Missing values verbosely ignored. bdp desired breakdown point (0 0.5). order compute gradient (order=1) gradient Hessian (order=2). cc cutoff value bisquare rho function. default, chosen yield consistent estimate Normal distribution. opts list options M-scale estimation algorithm, see mscale_algorithm_options() details.","code":""},{"path":"/reference/mscale_derivative.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the Gradient and Hessian of the M-Scale Function — mscale_derivative","text":"vector derivatives M-scale function, one per element x.","code":""},{"path":"/reference/pense.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute (Adaptive) Elastic Net S-Estimates of Regression — pense","title":"Compute (Adaptive) Elastic Net S-Estimates of Regression — pense","text":"Compute elastic net S-estimates (PENSE estimates) along grid penalization levels optional penalty loadings adaptive elastic net.","code":""},{"path":"/reference/pense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute (Adaptive) Elastic Net S-Estimates of Regression — pense","text":"","code":"pense(   x,   y,   alpha,   nlambda = 50,   nlambda_enpy = 10,   lambda,   lambda_min_ratio,   enpy_lambda,   penalty_loadings,   intercept = TRUE,   bdp = 0.25,   cc,   add_zero_based = TRUE,   enpy_specific = FALSE,   other_starts,   carry_forward = TRUE,   eps = 1e-06,   explore_solutions = 0,   explore_tol = 0.1,   explore_it = 5,   max_solutions = 5,   comparison_tol = sqrt(eps),   sparse = FALSE,   ncores = 1,   standardize = TRUE,   algorithm_opts = mm_algorithm_options(),   mscale_opts = mscale_algorithm_options(),   enpy_opts = enpy_options(),   ... )"},{"path":"/reference/pense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute (Adaptive) Elastic Net S-Estimates of Regression — pense","text":"x n p matrix numeric predictors. y vector response values length n. binary classification, y factor 2 levels. alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. Can vector several values, alpha = 0 mixed values. nlambda number penalization levels. nlambda_enpy number penalization levels EN-PY initial estimate computed. lambda optional user-supplied sequence penalization levels. given NULL, nlambda lambda_min_ratio ignored. lambda_min_ratio Smallest value penalization level fraction largest level (.e., smallest value coefficients zero). default depends sample size relative number variables alpha. observations variables available, default 1e-3 * alpha, otherwise 1e-2 * alpha. enpy_lambda optional user-supplied sequence penalization levels EN-PY initial estimates computed. given NULL, nlambda_enpy ignored. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. allowed alpha > 0. intercept include intercept model. bdp desired breakdown point estimator, 0.05 0.5. actual breakdown point may slightly larger/smaller avoid instabilities S-loss. cc tuning constant S-estimator. Default chosen based breakdown point bdp. affects estimated coefficients standardize=TRUE. Otherwise estimated scale residuals affected. add_zero_based also consider 0-based regularization path. See details description. enpy_specific use EN-PY initial estimates penalization level computed . See details description. other_starts list staring points, created starting_point(). output enpy_initial_estimates() given, starting points shared among penalization levels. Note starting point specific penalization level, penalization level added grid penalization levels (either manually specified grid lambda automatically generated grid size nlambda). standardize = TRUE, starting points also scaled. carry_forward carry best solutions forward next penalty level. eps numerical tolerance. explore_solutions number solutions keep exploration step. best explore_solutions iterated full numerical tolerance eps. 0, non-duplicated solutions kept. explore_tol, explore_it numerical tolerance maximum number iterations exploring possible solutions. tolerance (much) looser eps useful, number iterations also much smaller maximum number iterations given via algorithm_opts. explore_tol also used determine two solutions equal exploration stage. max_solutions retain max_solutions unique solutions per penalization level. comparison_tol numeric tolerance determine two solutions equal. comparison first done absolute difference value objective function solution. less comparison_tol, two solutions deemed equal squared difference intercepts less comparison_tol squared \\(L_2\\) norm difference vector less comparison_tol. sparse use sparse coefficient vectors. ncores number CPU cores use parallel. default, one CPU core used. supported platforms, case warning given. standardize logical flag standardize x variables prior fitting PENSE estimates. Coefficients always returned original scale. can fail variables large proportion single value (e.g., zero-inflated data). case, either compute standardize = FALSE standardize data manually. algorithm_opts options MM algorithm compute estimates. See mm_algorithm_options() details. mscale_opts options M-scale estimation. See mscale_algorithm_options() details. enpy_opts options ENPY initial estimates, created enpy_options() function. See enpy_initial_estimates() details. ... ignored.","code":""},{"path":"/reference/pense.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute (Adaptive) Elastic Net S-Estimates of Regression — pense","text":"list-like object following items alpha sequence alpha parameters. lambda list sequences penalization levels, one per alpha parameter. estimates list estimates. estimate contains following information: intercept intercept estimate. beta beta (slope) estimate. lambda penalization level estimate computed. alpha alpha hyper-parameter estimate computed. bdp chosen breakdown-point. objf_value value objective function solution. statuscode > 0 algorithm experienced issues computing estimate. status optional status message algorithm. bdp actual breakdown point used. call original call.","code":""},{"path":"/reference/pense.html","id":"strategies-for-using-starting-points","dir":"Reference","previous_headings":"","what":"Strategies for Using Starting Points","title":"Compute (Adaptive) Elastic Net S-Estimates of Regression — pense","text":"function supports several different strategies compute, use provided starting points optimizing PENSE objective function. Starting points computed internally can also supplied via other_starts. default, starting points computed internally EN-PY procedure penalization levels supplied enpy_lambda (automatically generated grid length nlambda_enpy). default, starting points computed EN-PY procedure shared penalization levels lambda (automatically generated grid length nlambda). starting points specific penalization level starting points' penalization level, set enpy_specific argument TRUE. addition EN-PY initial estimates, algorithm can also use \"0-based\" strategy add_zero_based = TRUE (default). , 0-vector used start optimization largest penalization level lambda. subsequent penalization levels, solution previous penalization level also used starting point. every penalization level, starting points explored using loose numerical tolerance explore_tol. best explore_solutions computed stringent numerical tolerance eps. Finally, best max_solutions retained carried forward starting points subsequent penalization level.","code":""},{"path":[]},{"path":"/reference/pense.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute (Adaptive) Elastic Net S-Estimates of Regression — pense","text":"","code":"# Compute the PENSE regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- pense(x, freeny$y, alpha = 0.5) plot(regpath)   # Extract the coefficients at a certain penalization level coef(regpath, lambda = regpath$lambda[[1]][[40]]) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -6.5082299             0.2510560            -0.6879670  #>          income.level      market.potential  #>             0.7090986             0.9409940   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- pense_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 2, cv_k = 4) plot(cv_results, se_mult = 1)   # Print a summary of the fit and the cross-validation results. summary(cv_results) #> PENSE fit with prediction performance estimated by 2 replications of 4-fold ris  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -4.7921541 #> X1           0.3338834 #> X2          -0.6140406 #> X3           0.6954769 #> X4           0.7316339 #> --- #>  #> Hyper-parameters: lambda=0.0003364066, alpha=0.5  # Extract the coefficients at the penalization level with # smallest prediction error ... coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -4.7921541             0.3338834            -0.6140406  #>          income.level      market.potential  #>             0.6954769             0.7316339  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. coef(cv_results, lambda = '1-se') #>           (Intercept) lag.quarterly.revenue           price.index  #>           -11.4754472             0.2265866            -0.5739724  #>          income.level      market.potential  #>             0.5417608             1.3768215"},{"path":"/reference/pense_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for (Adaptive) PENSE Estimates — pense_cv","title":"Cross-validation for (Adaptive) PENSE Estimates — pense_cv","text":"Perform (repeated) K-fold cross-validation pense(). adapense_cv() convenience wrapper compute adaptive PENSE estimates.","code":""},{"path":"/reference/pense_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for (Adaptive) PENSE Estimates — pense_cv","text":"","code":"pense_cv(   x,   y,   standardize = TRUE,   lambda,   cv_k,   cv_repl = 1,   cv_type = c(\"ris\", \"naive\"),   cv_metric = c(\"tau_size\", \"mape\", \"rmspe\", \"auroc\"),   ris_min_similarity = 0.5,   fit_all = TRUE,   fold_starts = c(\"full\", \"enpy\", \"both\"),   cv_algorithm_opts,   cl = NULL,   ... )  adapense_cv(x, y, alpha, alpha_preliminary = 0, exponent = 1, ...)"},{"path":"/reference/pense_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for (Adaptive) PENSE Estimates — pense_cv","text":"x n p matrix numeric predictors. y vector response values length n. binary classification, y factor 2 levels. standardize whether standardize x variables prior fitting PENSE estimates. Can also set \"cv_only\", case input data standardized, training data CV folds scaled match scaling input data. Coefficients always returned original scale. can fail variables large proportion single value (e.g., zero-inflated data). case, either compute standardize = FALSE standardize data manually. lambda optional user-supplied sequence penalization levels. given NULL, nlambda lambda_min_ratio ignored. cv_k number folds per cross-validation. cv_repl number cross-validation replications. cv_type kind cross-validation performed: robust information sharing (ris) standard (naive) CV. cv_metric cv_type='naive'. Either string specifying performance metric use, function evaluate prediction errors single CV replication. function, number arguments define data function receives. function takes single argument, called single numeric vector prediction errors. function takes two arguments, called predicted values first argument true values second argument. function must always return single numeric value quantifying prediction performance. order given values corresponds order input data. ris_min_similarity minimum average similarity CV solutions considered (0 1). CV solution satisfies lower bound, best CV solution used regardless similarity. fit_all cv_type='naive'. TRUE, fit model penalization levels. Can also combination \"min\" \"{x}-se\", case models penalization level smallest average CV accuracy, within {x} standard errors, respectively. Setting fit_all FALSE equivalent \"min\". Applies alpha value. fold_starts determine starting values cross-validation folds. \"full\" (default), use best solution fit full data starting value. implies fit_all=TRUE. \"enpy\" compute separate ENPY initial estimates fold. option \"\" uses . starts addition starts provided other_starts. cv_algorithm_opts Override algorithm options CV iterations. usually necessary, unless user wants change number solutions retained CV training data. cl parallel cluster. Can used combination ncores = 1. ... Arguments passed pense nlambda number penalization levels. lambda_min_ratio Smallest value penalization level fraction largest level (.e., smallest value coefficients zero). default depends sample size relative number variables alpha. observations variables available, default 1e-3 * alpha, otherwise 1e-2 * alpha. nlambda_enpy number penalization levels EN-PY initial estimate computed. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. allowed alpha > 0. enpy_lambda optional user-supplied sequence penalization levels EN-PY initial estimates computed. given NULL, nlambda_enpy ignored. other_starts list staring points, created starting_point(). output enpy_initial_estimates() given, starting points shared among penalization levels. Note starting point specific penalization level, penalization level added grid penalization levels (either manually specified grid lambda automatically generated grid size nlambda). standardize = TRUE, starting points also scaled. intercept include intercept model. bdp desired breakdown point estimator, 0.05 0.5. actual breakdown point may slightly larger/smaller avoid instabilities S-loss. cc tuning constant S-estimator. Default chosen based breakdown point bdp. affects estimated coefficients standardize=TRUE. Otherwise estimated scale residuals affected. eps numerical tolerance. explore_solutions number solutions keep exploration step. best explore_solutions iterated full numerical tolerance eps. 0, non-duplicated solutions kept. explore_tol,explore_it numerical tolerance maximum number iterations exploring possible solutions. tolerance (much) looser eps useful, number iterations also much smaller maximum number iterations given via algorithm_opts. explore_tol also used determine two solutions equal exploration stage. max_solutions retain max_solutions unique solutions per penalization level. comparison_tol numeric tolerance determine two solutions equal. comparison first done absolute difference value objective function solution. less comparison_tol, two solutions deemed equal squared difference intercepts less comparison_tol squared \\(L_2\\) norm difference vector less comparison_tol. add_zero_based also consider 0-based regularization path. See details description. enpy_specific use EN-PY initial estimates penalization level computed . See details description. carry_forward carry best solutions forward next penalty level. sparse use sparse coefficient vectors. ncores number CPU cores use parallel. default, one CPU core used. supported platforms, case warning given. algorithm_opts options MM algorithm compute estimates. See mm_algorithm_options() details. mscale_opts options M-scale estimation. See mscale_algorithm_options() details. enpy_opts options ENPY initial estimates, created enpy_options() function. See enpy_initial_estimates() details. alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. Can vector several values, alpha = 0 mixed values. alpha_preliminary alpha parameter preliminary estimate. exponent exponent computing penalty loadings based preliminary estimate.","code":""},{"path":"/reference/pense_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for (Adaptive) PENSE Estimates — pense_cv","text":"list-like object components returned pense(), plus following: cvres data frame average cross-validated performance. list-like object returned pense_cv() plus following preliminary CV results preliminary estimate. exponent exponent used compute penalty loadings. penalty_loadings penalty loadings used adaptive PENSE estimate.","code":""},{"path":"/reference/pense_cv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for (Adaptive) PENSE Estimates — pense_cv","text":"built-CV metrics \"tau_size\" \\(\\tau\\)-size prediction error, computed tau_size() (default). \"mape\" Median absolute prediction error. \"rmspe\" Root mean squared prediction error. \"auroc\" Area receiver operator characteristic curve (actually 1 - AUROC). sensible binary responses. adapense_cv() convenience wrapper performs 3 steps: compute preliminary estimates via pense_cv(..., alpha = alpha_preliminary), computes penalty loadings estimate beta best prediction performance adapense_loadings = 1 / abs(beta)^exponent, compute adaptive PENSE estimates via pense_cv(..., penalty_loadings = adapense_loadings).","code":""},{"path":[]},{"path":"/reference/pense_cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for (Adaptive) PENSE Estimates — pense_cv","text":"","code":"# Compute the adaptive PENSE regularization path for Freeny's # revenue data (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  ## Either use the convenience function directly ... set.seed(123) ada_convenience <- adapense_cv(x, freeny$y, alpha = 0.5,                                cv_repl = 2, cv_k = 4)  ## ... or compute the steps manually: # Step 1: Compute preliminary estimates with CV set.seed(123) preliminary_estimate <- pense_cv(x, freeny$y, alpha = 0,                                  cv_repl = 2, cv_k = 4) plot(preliminary_estimate, se_mult = 1)  # Step 2: Use the coefficients with best prediction performance # to define the penalty loadings: prelim_coefs <- coef(preliminary_estimate, lambda = 'min') pen_loadings <- 1 / abs(prelim_coefs[-1])  # Step 3: Compute the adaptive PENSE estimates and estimate # their prediction performance. set.seed(123) ada_manual <- pense_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 2, cv_k = 4,                        penalty_loadings = pen_loadings)  # Visualize the prediction performance and coefficient path of # the adaptive PENSE estimates (manual vs. automatic) def.par <- par(no.readonly = TRUE) layout(matrix(1:4, ncol = 2, byrow = TRUE)) plot(ada_convenience$preliminary) plot(preliminary_estimate) plot(ada_convenience) plot(ada_manual)  par(def.par)"},{"path":"/reference/plot.pense_cvfit.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for Penalized Estimates With Cross-Validation — plot.pense_cvfit","title":"Plot Method for Penalized Estimates With Cross-Validation — plot.pense_cvfit","text":"Plot cross-validation performance coefficient path fitted penalized elastic net S- LS-estimates regression.","code":""},{"path":"/reference/plot.pense_cvfit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for Penalized Estimates With Cross-Validation — plot.pense_cvfit","text":"","code":"# S3 method for class 'pense_cvfit' plot(x, what = c(\"cv\", \"coef.path\"), alpha = NULL, se_mult = 1, ...)"},{"path":"/reference/plot.pense_cvfit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for Penalized Estimates With Cross-Validation — plot.pense_cvfit","text":"x fitted estimates cross-validation information. plot either CV performance coefficient path. alpha = \"cv\", CV performance fits matching alpha plotted. case alpha missing NULL, fits x plotted. = \"coef.path\", plot coefficient path fit given hyper-parameter value , case alpha missing, first value x$alpha. se_mult plotting CV performance, multiplier estimated SE. ... currently ignored.","code":""},{"path":[]},{"path":"/reference/plot.pense_cvfit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for Penalized Estimates With Cross-Validation — plot.pense_cvfit","text":"","code":"# Compute the PENSE regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- pense(x, freeny$y, alpha = 0.5) plot(regpath)   # Extract the coefficients at a certain penalization level coef(regpath, lambda = regpath$lambda[[1]][[40]]) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -6.5082299             0.2510560            -0.6879670  #>          income.level      market.potential  #>             0.7090986             0.9409940   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- pense_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 2, cv_k = 4) plot(cv_results, se_mult = 1)   # Print a summary of the fit and the cross-validation results. summary(cv_results) #> PENSE fit with prediction performance estimated by 2 replications of 4-fold ris  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -4.7921541 #> X1           0.3338834 #> X2          -0.6140406 #> X3           0.6954769 #> X4           0.7316339 #> --- #>  #> Hyper-parameters: lambda=0.0003364066, alpha=0.5  # Extract the coefficients at the penalization level with # smallest prediction error ... coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -4.7921541             0.3338834            -0.6140406  #>          income.level      market.potential  #>             0.6954769             0.7316339  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. coef(cv_results, lambda = '1-se') #>           (Intercept) lag.quarterly.revenue           price.index  #>           -11.4754472             0.2265866            -0.5739724  #>          income.level      market.potential  #>             0.5417608             1.3768215"},{"path":"/reference/plot.pense_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for Penalized Estimates — plot.pense_fit","title":"Plot Method for Penalized Estimates — plot.pense_fit","text":"Plot coefficient path fitted penalized elastic net S- LS-estimates regression.","code":""},{"path":"/reference/plot.pense_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for Penalized Estimates — plot.pense_fit","text":"","code":"# S3 method for class 'pense_fit' plot(x, alpha, ...)"},{"path":"/reference/plot.pense_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for Penalized Estimates — plot.pense_fit","text":"x fitted estimates. alpha Plot coefficient path fit given hyper-parameter value. missing NULL, first value x$alpha used. ... currently ignored.","code":""},{"path":[]},{"path":"/reference/plot.pense_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for Penalized Estimates — plot.pense_fit","text":"","code":"# Compute the PENSE regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- pense(x, freeny$y, alpha = 0.5) plot(regpath)   # Extract the coefficients at a certain penalization level coef(regpath, lambda = regpath$lambda[[1]][[40]]) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -6.5082299             0.2510560            -0.6879670  #>          income.level      market.potential  #>             0.7090986             0.9409940   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- pense_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 2, cv_k = 4) plot(cv_results, se_mult = 1)   # Print a summary of the fit and the cross-validation results. summary(cv_results) #> PENSE fit with prediction performance estimated by 2 replications of 4-fold ris  #> cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -4.7921541 #> X1           0.3338834 #> X2          -0.6140406 #> X3           0.6954769 #> X4           0.7316339 #> --- #>  #> Hyper-parameters: lambda=0.0003364066, alpha=0.5  # Extract the coefficients at the penalization level with # smallest prediction error ... coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -4.7921541             0.3338834            -0.6140406  #>          income.level      market.potential  #>             0.6954769             0.7316339  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. coef(cv_results, lambda = '1-se') #>           (Intercept) lag.quarterly.revenue           price.index  #>           -11.4754472             0.2265866            -0.5739724  #>          income.level      market.potential  #>             0.5417608             1.3768215"},{"path":"/reference/predict.pense_cvfit.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for PENSE Fits — predict.pense_cvfit","title":"Predict Method for PENSE Fits — predict.pense_cvfit","text":"Predict response values using PENSE (LS-EN) regularization path hyper-parameters chosen cross-validation.","code":""},{"path":"/reference/predict.pense_cvfit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for PENSE Fits — predict.pense_cvfit","text":"","code":"# S3 method for class 'pense_cvfit' predict(object, newdata, alpha = NULL, lambda = \"min\", se_mult = 1, ...)"},{"path":"/reference/predict.pense_cvfit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for PENSE Fits — predict.pense_cvfit","text":"object PENSE cross-validated hyper-parameters extract coefficients . newdata optional matrix new predictor values. missing, fitted values computed. alpha Either single number NULL (default). given, fits given alpha value considered. lambda numeric value object fit multiple alpha values value provided, first value object$alpha used warning. lambda either string specifying penalty level use (\"min\", \"se\", \"{m}-se\") single numeric value penalty parameter. See details. se_mult lambda = \"se\", multiple standard errors tolerate. ... currently used.","code":""},{"path":"/reference/predict.pense_cvfit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for PENSE Fits — predict.pense_cvfit","text":"numeric vector residuals given penalization level.","code":""},{"path":"/reference/predict.pense_cvfit.html","id":"hyper-parameters","dir":"Reference","previous_headings":"","what":"Hyper-parameters","title":"Predict Method for PENSE Fits — predict.pense_cvfit","text":"lambda = \"{m}-se\" object contains fitted estimates every penalization level sequence, use fit parsimonious model prediction performance statistically indistinguishable best model. determined model prediction performance within m * cv_se best model. lambda = \"se\", multiplier m taken se_mult. default alpha hyper-parameters available fitted object considered. can overridden supplying one multiple values parameter alpha. example, lambda = \"1-se\" alpha contains two values, \"1-SE\" rule applied individually alpha value, fit better prediction error considered. case lambda number object fit several alpha hyper-parameters, alpha must also given, first value object$alpha used warning.","code":""},{"path":[]},{"path":"/reference/predict.pense_cvfit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for PENSE Fits — predict.pense_cvfit","text":"","code":"# Compute the LS-EN regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- elnet(x, freeny$y, alpha = 0.75)  # Predict the response using a specific penalization level predict(regpath, newdata = freeny[1:5, 2:5],         lambda = regpath$lambda[[1]][[10]]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 9.071638 9.075877 9.082341 9.091051 9.103643   # Extract the residuals at a certain penalization level residuals(regpath, lambda = regpath$lambda[[1]][[5]]) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.396169224 -0.398919454 -0.378220695 #> 1963 -0.384593892 -0.294338479 -0.278030430 -0.259140970 #> 1964 -0.265543684 -0.219857551 -0.206023748 -0.173545125 #> 1965 -0.192086388 -0.146253918 -0.133786817 -0.096671191 #> 1966 -0.090347926 -0.044601264 -0.027307275 -0.015531362 #> 1967  0.006739697  0.037235389  0.038869333  0.071407419 #> 1968  0.086275062  0.102036759  0.141643505  0.167310507 #> 1969  0.175437660  0.211500792  0.222942549  0.260029329 #> 1970  0.247870647  0.296181577  0.294470169  0.278217170 #> 1971  0.306686546  0.334684047  0.353729154  0.367702082  # Select penalization level via cross-validation set.seed(123) cv_results <- elnet_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 10, cv_k = 4)  # Predict the response using the \"best\" penalization level predict(cv_results, newdata = freeny[1:5, 2:5]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 8.795162 8.807070 8.824535 8.842175 8.882970   # Extract the residuals at the \"best\" penalization level residuals(cv_results) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.002801588 -0.015699794 -0.009674689 #> 1963 -0.029165027  0.024539756  0.012794692  0.013400637 #> 1964 -0.014177641  0.006727561 -0.001786569  0.012847301 #> 1965 -0.026214283 -0.003738367 -0.010414370 -0.002497682 #> 1966 -0.014953173  0.014923561  0.008857587  0.002955549 #> 1967 -0.002438578  0.011336771 -0.004817002 -0.002203485 #> 1968 -0.014622332 -0.016038521  0.004783442  0.009509723 #> 1969  0.008935501  0.025135859  0.011008309  0.028171208 #> 1970 -0.021812388  0.015300404 -0.005936032 -0.018484409 #> 1971 -0.011338929  0.005850584  0.003783641  0.007952774 # Extract the residuals at a more parsimonious penalization level residuals(cv_results, lambda = \"1.5-se\") #>               Qtr1          Qtr2          Qtr3          Qtr4 #> 1962               -0.0133914763 -0.0253025812 -0.0180073467 #> 1963 -0.0375085950  0.0197904960  0.0063674275  0.0071084444 #> 1964 -0.0199445085  0.0028559476 -0.0059886764  0.0089958759 #> 1965 -0.0300906249 -0.0052788324 -0.0128420949 -0.0036364879 #> 1966 -0.0166186511  0.0137904162  0.0073750895  0.0016121587 #> 1967 -0.0021618673  0.0116385091 -0.0046227113  0.0002016235 #> 1968 -0.0117185912 -0.0127504673  0.0088610048  0.0131305182 #> 1969  0.0114094522  0.0285319822  0.0147126545  0.0325247699 #> 1970 -0.0158575338  0.0222103711  0.0004937030 -0.0127802705 #> 1971 -0.0035765461  0.0133307584  0.0117139290  0.0154227310"},{"path":"/reference/predict.pense_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for PENSE Fits — predict.pense_fit","title":"Predict Method for PENSE Fits — predict.pense_fit","text":"Predict response values using PENSE (LS-EN) regularization path fitted pense(), regmest() elnet().","code":""},{"path":"/reference/predict.pense_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for PENSE Fits — predict.pense_fit","text":"","code":"# S3 method for class 'pense_fit' predict(object, newdata, alpha = NULL, lambda, ...)"},{"path":"/reference/predict.pense_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for PENSE Fits — predict.pense_fit","text":"object PENSE regularization path extract residuals . newdata optional matrix new predictor values. missing, fitted values computed. alpha Either single number NULL (default). given, fits given alpha value considered. object fit multiple alpha values, value provided, first value object$alpha used warning. lambda single number penalty level. ... currently used.","code":""},{"path":"/reference/predict.pense_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for PENSE Fits — predict.pense_fit","text":"numeric vector residuals given penalization level.","code":""},{"path":[]},{"path":"/reference/predict.pense_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for PENSE Fits — predict.pense_fit","text":"","code":"# Compute the LS-EN regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- elnet(x, freeny$y, alpha = 0.75)  # Predict the response using a specific penalization level predict(regpath, newdata = freeny[1:5, 2:5],         lambda = regpath$lambda[[1]][[10]]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 9.071638 9.075877 9.082341 9.091051 9.103643   # Extract the residuals at a certain penalization level residuals(regpath, lambda = regpath$lambda[[1]][[5]]) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.396169224 -0.398919454 -0.378220695 #> 1963 -0.384593892 -0.294338479 -0.278030430 -0.259140970 #> 1964 -0.265543684 -0.219857551 -0.206023748 -0.173545125 #> 1965 -0.192086388 -0.146253918 -0.133786817 -0.096671191 #> 1966 -0.090347926 -0.044601264 -0.027307275 -0.015531362 #> 1967  0.006739697  0.037235389  0.038869333  0.071407419 #> 1968  0.086275062  0.102036759  0.141643505  0.167310507 #> 1969  0.175437660  0.211500792  0.222942549  0.260029329 #> 1970  0.247870647  0.296181577  0.294470169  0.278217170 #> 1971  0.306686546  0.334684047  0.353729154  0.367702082  # Select penalization level via cross-validation set.seed(123) cv_results <- elnet_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 10, cv_k = 4)  # Predict the response using the \"best\" penalization level predict(cv_results, newdata = freeny[1:5, 2:5]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 8.795162 8.807070 8.824535 8.842175 8.882970   # Extract the residuals at the \"best\" penalization level residuals(cv_results) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.002801588 -0.015699794 -0.009674689 #> 1963 -0.029165027  0.024539756  0.012794692  0.013400637 #> 1964 -0.014177641  0.006727561 -0.001786569  0.012847301 #> 1965 -0.026214283 -0.003738367 -0.010414370 -0.002497682 #> 1966 -0.014953173  0.014923561  0.008857587  0.002955549 #> 1967 -0.002438578  0.011336771 -0.004817002 -0.002203485 #> 1968 -0.014622332 -0.016038521  0.004783442  0.009509723 #> 1969  0.008935501  0.025135859  0.011008309  0.028171208 #> 1970 -0.021812388  0.015300404 -0.005936032 -0.018484409 #> 1971 -0.011338929  0.005850584  0.003783641  0.007952774 # Extract the residuals at a more parsimonious penalization level residuals(cv_results, lambda = \"1.5-se\") #>               Qtr1          Qtr2          Qtr3          Qtr4 #> 1962               -0.0133914763 -0.0253025812 -0.0180073467 #> 1963 -0.0375085950  0.0197904960  0.0063674275  0.0071084444 #> 1964 -0.0199445085  0.0028559476 -0.0059886764  0.0089958759 #> 1965 -0.0300906249 -0.0052788324 -0.0128420949 -0.0036364879 #> 1966 -0.0166186511  0.0137904162  0.0073750895  0.0016121587 #> 1967 -0.0021618673  0.0116385091 -0.0046227113  0.0002016235 #> 1968 -0.0117185912 -0.0127504673  0.0088610048  0.0131305182 #> 1969  0.0114094522  0.0285319822  0.0147126545  0.0325247699 #> 1970 -0.0158575338  0.0222103711  0.0004937030 -0.0127802705 #> 1971 -0.0035765461  0.0133307584  0.0117139290  0.0154227310"},{"path":"/reference/prediction_performance.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction Performance of Adaptive PENSE Fits — prediction_performance","title":"Prediction Performance of Adaptive PENSE Fits — prediction_performance","text":"Extract prediction performance one (adaptive) PENSE fits.","code":""},{"path":"/reference/prediction_performance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction Performance of Adaptive PENSE Fits — prediction_performance","text":"","code":"prediction_performance(..., alpha = NULL, lambda = \"min\", se_mult = 1)  # S3 method for class 'pense_pred_perf' print(x, ...)"},{"path":"/reference/prediction_performance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction Performance of Adaptive PENSE Fits — prediction_performance","text":"... one (adaptive) PENSE fits cross-validation information. alpha Either numeric vector NULL (default). given, fits given alpha value considered. lambda numeric value object fit multiple alpha values, parameter alpha must missing. lambda either string specifying penalty level use (\"min\", \"se\", \"{x}-se\") single numeric value penalty parameter. See details. se_mult lambda = \"se\", multiple standard errors tolerate. x object information prediction performance created prediction_performance().","code":""},{"path":"/reference/prediction_performance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction Performance of Adaptive PENSE Fits — prediction_performance","text":"data frame details prediction performance given PENSE fits. data frame custom print method summarizing prediction performances.","code":""},{"path":"/reference/prediction_performance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prediction Performance of Adaptive PENSE Fits — prediction_performance","text":"lambda = \"se\" cross-validation performed multiple replications, use penalty level whit prediction performance within se_mult best prediction performance.","code":""},{"path":[]},{"path":"/reference/prinsens.html","id":null,"dir":"Reference","previous_headings":"","what":"Principal Sensitivity Components — prinsens","title":"Principal Sensitivity Components — prinsens","text":"Compute Principal Sensitivity Components Elastic Net Regression","code":""},{"path":"/reference/prinsens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Principal Sensitivity Components — prinsens","text":"","code":"prinsens(   x,   y,   alpha,   lambda,   intercept = TRUE,   penalty_loadings,   en_algorithm_opts,   eps = 1e-06,   sparse = FALSE,   ncores = 1L )"},{"path":"/reference/prinsens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Principal Sensitivity Components — prinsens","text":"x n p matrix numeric predictors. y vector response values length n. alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. Can vector several values, alpha = 0 mixed values. lambda optional user-supplied sequence penalization levels. given NULL, nlambda lambda_min_ratio ignored. intercept include intercept model. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. allowed alpha > 0. en_algorithm_opts options LS-EN algorithm. See en_algorithm_options details. eps numerical tolerance. sparse use sparse coefficient vectors. ncores number CPU cores use parallel. default, one CPU core used. supported platforms, case warning given.","code":""},{"path":"/reference/prinsens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Principal Sensitivity Components — prinsens","text":"list principal sensitivity components, one per element lambda. PSC list items lambda, alpha, pscs.","code":""},{"path":"/reference/prinsens.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Principal Sensitivity Components — prinsens","text":"Cohen Freue, G.V.; Kepplinger, D.; Salibián-Barrera, M.; Smucler, E. Robust elastic net estimators variable selection identification proteomic biomarkers. Ann. Appl. Stat. 13 (2019), . 4, 2065–2090 doi:10.1214/19-AOAS1269 Pena, D., Yohai, V.J. Fast Procedure Outlier Diagnostics Large Regression Problems. J. Amer. Statist. Assoc. 94 (1999). . 446, 434–445. doi:10.2307/2670164","code":""},{"path":[]},{"path":"/reference/print.nsoptim_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Metrics — print.nsoptim_metrics","title":"Print Metrics — print.nsoptim_metrics","text":"Pretty-print list metrics optimization algorithm (pense built metrics enabled).","code":""},{"path":"/reference/print.nsoptim_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Metrics — print.nsoptim_metrics","text":"","code":"# S3 method for class 'nsoptim_metrics' print(x, max_level = NA, ...)"},{"path":"/reference/print.nsoptim_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Metrics — print.nsoptim_metrics","text":"x metrics object printing. max_level maximum level printing applied printing nested metrics.","code":""},{"path":"/reference/regmest.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute (Adaptive) Elastic Net M-Estimates of Regression — regmest","title":"Compute (Adaptive) Elastic Net M-Estimates of Regression — regmest","text":"Compute elastic net M-estimates along grid penalization levels optional penalty loadings adaptive elastic net.","code":""},{"path":"/reference/regmest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute (Adaptive) Elastic Net M-Estimates of Regression — regmest","text":"","code":"regmest(   x,   y,   alpha,   nlambda = 50,   lambda,   lambda_min_ratio,   scale,   starting_points,   penalty_loadings,   intercept = TRUE,   eff = 0.9,   rho = \"mopt\",   cc,   eps = 1e-06,   explore_solutions = 10,   explore_tol = 0.1,   max_solutions = 1,   comparison_tol = sqrt(eps),   sparse = FALSE,   ncores = 1,   standardize = TRUE,   algorithm_opts = mm_algorithm_options(),   add_zero_based = TRUE,   mscale_bdp = 0.25,   mscale_opts = mscale_algorithm_options() )"},{"path":"/reference/regmest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute (Adaptive) Elastic Net M-Estimates of Regression — regmest","text":"x n p matrix numeric predictors. y vector response values length n. binary classification, y factor 2 levels. alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. nlambda number penalization levels. lambda optional user-supplied sequence penalization levels. given NULL, nlambda lambda_min_ratio ignored. lambda_min_ratio Smallest value penalization level fraction largest level (.e., smallest value coefficients zero). default depends sample size relative number variables alpha. observations variables available, default 1e-3 * alpha, otherwise 1e-2 * alpha. scale fixed scale residuals. starting_points list staring points, created starting_point(). starting points shared among penalization levels. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. allowed alpha > 0. intercept include intercept model. eff desired asymptotic efficiency M-estimator Normal model. rho \\(\\rho\\) function use (see rho_function() list supported options). cc manually specified cutoff constant chosen \\(\\rho\\) function. specified, overrides eff argument. eps numerical tolerance. explore_solutions number solutions compute desired precision eps. explore_tol numerical tolerance exploring possible solutions. (much) looser eps useful. max_solutions retain max_solutions unique solutions per penalization level. comparison_tol numeric tolerance determine two solutions equal. comparison first done absolute difference value objective function solution. less comparison_tol, two solutions deemed equal squared difference intercepts less comparison_tol squared \\(L_2\\) norm difference vector less comparison_tol. sparse use sparse coefficient vectors. ncores number CPU cores use parallel. default, one CPU core used. supported platforms, case warning given. standardize logical flag standardize x variables prior fitting M-estimates. Coefficients always returned original scale. can fail variables large proportion single value (e.g., zero-inflated data). case, either compute standardize = FALSE standardize data manually. algorithm_opts options MM algorithm compute estimates. See mm_algorithm_options() details. add_zero_based also consider 0-based regularization path addition given starting points. mscale_bdp, mscale_opts options M-scale estimate used standardize predictors (standardize = TRUE).","code":""},{"path":"/reference/regmest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute (Adaptive) Elastic Net M-Estimates of Regression — regmest","text":"list-like object following items alpha sequence alpha parameters. lambda list sequences penalization levels, one per alpha parameter. scale used scale residuals. estimates list estimates. estimate contains following information: intercept intercept estimate. beta beta (slope) estimate. lambda penalization level estimate computed. alpha alpha hyper-parameter estimate computed. objf_value value objective function solution. statuscode > 0 algorithm experienced issues computing estimate. status optional status message algorithm. call original call.","code":""},{"path":[]},{"path":"/reference/regmest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute (Adaptive) Elastic Net M-Estimates of Regression — regmest","text":"","code":"# Compute the PENSE regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- regmest(x, freeny$y, alpha = c(0.5, 0.85), scale = 2) plot(regpath)   # Extract the coefficients at a certain penalization level coef(regpath, alpha = 0.85, lambda = regpath$lambda[[2]][[40]]) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.9619795             0.1375927            -0.7438982  #>          income.level      market.potential  #>             0.7590060             1.2820779   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- regmest_cv(x, freeny$y, alpha = c(0.5, 0.85), scale = 2,                          cv_repl = 2, cv_k = 4) plot(cv_results, se_mult = 1)   # Print a summary of the fit and the cross-validation results. summary(cv_results) #> Regularized M fit with prediction performance estimated by replications of  #> 4-fold cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -9.1008312 #> X1           0.1900746 #> X2          -0.6944793 #> X3           0.7193090 #> X4           1.1802400 #> --- #>  #> Hyper-parameters: lambda=0.001012326, alpha=0.5  # Extract the coefficients at the penalization level with # smallest prediction error ... coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.1008312             0.1900746            -0.6944793  #>          income.level      market.potential  #>             0.7193090             1.1802400  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. coef(cv_results, lambda = '1-se') #>           (Intercept) lag.quarterly.revenue           price.index  #>            -8.7197254             0.2111671            -0.6634113  #>          income.level      market.potential  #>             0.6986596             1.1349456"},{"path":"/reference/regmest_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for (Adaptive) Elastic Net M-Estimates — regmest_cv","title":"Cross-validation for (Adaptive) Elastic Net M-Estimates — regmest_cv","text":"Perform (repeated) K-fold cross-validation regmest(). adamest_cv() convenience wrapper compute adaptive elastic-net M-estimates.","code":""},{"path":"/reference/regmest_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for (Adaptive) Elastic Net M-Estimates — regmest_cv","text":"","code":"regmest_cv(   x,   y,   standardize = TRUE,   lambda,   cv_k,   cv_repl = 1,   cv_type = \"naive\",   cv_metric = c(\"tau_size\", \"mape\", \"rmspe\", \"auroc\"),   fit_all = TRUE,   cl = NULL,   ... )  adamest_cv(x, y, alpha, alpha_preliminary = 0, exponent = 1, ...)"},{"path":"/reference/regmest_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for (Adaptive) Elastic Net M-Estimates — regmest_cv","text":"x n p matrix numeric predictors. y vector response values length n. binary classification, y factor 2 levels. standardize whether standardize x variables prior fitting PENSE estimates. Can also set \"cv_only\", case input data standardized, training data CV folds scaled match scaling input data. Coefficients always returned original scale. can fail variables large proportion single value (e.g., zero-inflated data). case, either compute standardize = FALSE standardize data manually. lambda optional user-supplied sequence penalization levels. given NULL, nlambda lambda_min_ratio ignored. cv_k number folds per cross-validation. cv_repl number cross-validation replications. cv_type kind cross-validation performed: robust information sharing (ris) standard (naive) CV. cv_metric cv_type='naive'. Either string specifying performance metric use, function evaluate prediction errors single CV replication. function, number arguments define data function receives. function takes single argument, called single numeric vector prediction errors. function takes two arguments, called predicted values first argument true values second argument. function must always return single numeric value quantifying prediction performance. order given values corresponds order input data. fit_all cv_type='naive'. TRUE, fit model penalization levels. Can also combination \"min\" \"{x}-se\", case models penalization level smallest average CV accuracy, within {x} standard errors, respectively. Setting fit_all FALSE equivalent \"min\". Applies alpha value. cl parallel cluster. Can used combination ncores = 1. ... Arguments passed regmest scale fixed scale residuals. nlambda number penalization levels. lambda_min_ratio Smallest value penalization level fraction largest level (.e., smallest value coefficients zero). default depends sample size relative number variables alpha. observations variables available, default 1e-3 * alpha, otherwise 1e-2 * alpha. penalty_loadings vector positive penalty loadings (.k.. weights) different penalization coefficient. allowed alpha > 0. starting_points list staring points, created starting_point(). starting points shared among penalization levels. intercept include intercept model. add_zero_based also consider 0-based regularization path addition given starting points. rho \\(\\rho\\) function use (see rho_function() list supported options). eff desired asymptotic efficiency M-estimator Normal model. cc manually specified cutoff constant chosen \\(\\rho\\) function. specified, overrides eff argument. eps numerical tolerance. explore_solutions number solutions compute desired precision eps. explore_tol numerical tolerance exploring possible solutions. (much) looser eps useful. max_solutions retain max_solutions unique solutions per penalization level. comparison_tol numeric tolerance determine two solutions equal. comparison first done absolute difference value objective function solution. less comparison_tol, two solutions deemed equal squared difference intercepts less comparison_tol squared \\(L_2\\) norm difference vector less comparison_tol. sparse use sparse coefficient vectors. ncores number CPU cores use parallel. default, one CPU core used. supported platforms, case warning given. algorithm_opts options MM algorithm compute estimates. See mm_algorithm_options() details. mscale_bdp,mscale_opts options M-scale estimate used standardize predictors (standardize = TRUE). alpha elastic net penalty mixing parameter \\(0 \\le \\alpha \\le 1\\). alpha = 1 LASSO penalty, alpha = 0 Ridge penalty. alpha_preliminary alpha parameter preliminary estimate. exponent exponent computing penalty loadings based preliminary estimate.","code":""},{"path":"/reference/regmest_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for (Adaptive) Elastic Net M-Estimates — regmest_cv","text":"list-like object returned regmest(), plus following components: cvres data frame average cross-validated performance. list-like object returned adamest_cv() plus following components: exponent value exponent. preliminary CV results preliminary estimate. penalty_loadings penalty loadings used adaptive elastic net M-estimate.","code":""},{"path":"/reference/regmest_cv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for (Adaptive) Elastic Net M-Estimates — regmest_cv","text":"built-CV metrics \"tau_size\" \\(\\tau\\)-size prediction error, computed tau_size() (default). \"mape\" Median absolute prediction error. \"rmspe\" Root mean squared prediction error. \"auroc\" Area receiver operator characteristic curve (actually 1 - AUROC). sensible binary responses. adamest_cv() convenience wrapper performs 3 steps: compute preliminary estimates via regmest_cv(..., alpha = alpha_preliminary), computes penalty loadings estimate beta best prediction performance adamest_loadings = 1 / abs(beta)^exponent, compute adaptive PENSE estimates via regmest_cv(..., penalty_loadings = adamest_loadings).","code":""},{"path":[]},{"path":"/reference/regmest_cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for (Adaptive) Elastic Net M-Estimates — regmest_cv","text":"","code":"# Compute the PENSE regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- regmest(x, freeny$y, alpha = c(0.5, 0.85), scale = 2) plot(regpath)   # Extract the coefficients at a certain penalization level coef(regpath, alpha = 0.85, lambda = regpath$lambda[[2]][[40]]) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.9619795             0.1375927            -0.7438982  #>          income.level      market.potential  #>             0.7590060             1.2820779   # What penalization level leads to good prediction performance? set.seed(123) cv_results <- regmest_cv(x, freeny$y, alpha = c(0.5, 0.85), scale = 2,                          cv_repl = 2, cv_k = 4) plot(cv_results, se_mult = 1)   # Print a summary of the fit and the cross-validation results. summary(cv_results) #> Regularized M fit with prediction performance estimated by replications of  #> 4-fold cross-validation. #>  #> 4 out of 4 predictors have non-zero coefficients: #>  #>               Estimate #> (Intercept) -9.1008312 #> X1           0.1900746 #> X2          -0.6944793 #> X3           0.7193090 #> X4           1.1802400 #> --- #>  #> Hyper-parameters: lambda=0.001012326, alpha=0.5  # Extract the coefficients at the penalization level with # smallest prediction error ... coef(cv_results) #>           (Intercept) lag.quarterly.revenue           price.index  #>            -9.1008312             0.1900746            -0.6944793  #>          income.level      market.potential  #>             0.7193090             1.1802400  # ... or at the penalization level with prediction error # statistically indistinguishable from the minimum. coef(cv_results, lambda = '1-se') #>           (Intercept) lag.quarterly.revenue           price.index  #>            -8.7197254             0.2111671            -0.6634113  #>          income.level      market.potential  #>             0.6986596             1.1349456"},{"path":"/reference/residuals.pense_cvfit.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Residuals — residuals.pense_cvfit","title":"Extract Residuals — residuals.pense_cvfit","text":"Extract residuals PENSE (LS-EN) regularization path hyper-parameters chosen cross-validation.","code":""},{"path":"/reference/residuals.pense_cvfit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Residuals — residuals.pense_cvfit","text":"","code":"# S3 method for class 'pense_cvfit' residuals(object, alpha = NULL, lambda = \"min\", se_mult = 1, ...)"},{"path":"/reference/residuals.pense_cvfit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Residuals — residuals.pense_cvfit","text":"object PENSE cross-validated hyper-parameters extract coefficients . alpha Either single number NULL (default). given, fits given alpha value considered. lambda numeric value object fit multiple alpha values value provided, first value object$alpha used warning. lambda either string specifying penalty level use (\"min\", \"se\", \"{m}-se\") single numeric value penalty parameter. See details. se_mult lambda = \"se\", multiple standard errors tolerate. ... currently used.","code":""},{"path":"/reference/residuals.pense_cvfit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Residuals — residuals.pense_cvfit","text":"numeric vector residuals given penalization level.","code":""},{"path":"/reference/residuals.pense_cvfit.html","id":"hyper-parameters","dir":"Reference","previous_headings":"","what":"Hyper-parameters","title":"Extract Residuals — residuals.pense_cvfit","text":"lambda = \"{m}-se\" object contains fitted estimates every penalization level sequence, use fit parsimonious model prediction performance statistically indistinguishable best model. determined model prediction performance within m * cv_se best model. lambda = \"se\", multiplier m taken se_mult. default alpha hyper-parameters available fitted object considered. can overridden supplying one multiple values parameter alpha. example, lambda = \"1-se\" alpha contains two values, \"1-SE\" rule applied individually alpha value, fit better prediction error considered. case lambda number object fit several alpha hyper-parameters, alpha must also given, first value object$alpha used warning.","code":""},{"path":[]},{"path":"/reference/residuals.pense_cvfit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Residuals — residuals.pense_cvfit","text":"","code":"# Compute the LS-EN regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- elnet(x, freeny$y, alpha = 0.75)  # Predict the response using a specific penalization level predict(regpath, newdata = freeny[1:5, 2:5],         lambda = regpath$lambda[[1]][[10]]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 9.071638 9.075877 9.082341 9.091051 9.103643   # Extract the residuals at a certain penalization level residuals(regpath, lambda = regpath$lambda[[1]][[5]]) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.396169224 -0.398919454 -0.378220695 #> 1963 -0.384593892 -0.294338479 -0.278030430 -0.259140970 #> 1964 -0.265543684 -0.219857551 -0.206023748 -0.173545125 #> 1965 -0.192086388 -0.146253918 -0.133786817 -0.096671191 #> 1966 -0.090347926 -0.044601264 -0.027307275 -0.015531362 #> 1967  0.006739697  0.037235389  0.038869333  0.071407419 #> 1968  0.086275062  0.102036759  0.141643505  0.167310507 #> 1969  0.175437660  0.211500792  0.222942549  0.260029329 #> 1970  0.247870647  0.296181577  0.294470169  0.278217170 #> 1971  0.306686546  0.334684047  0.353729154  0.367702082  # Select penalization level via cross-validation set.seed(123) cv_results <- elnet_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 10, cv_k = 4)  # Predict the response using the \"best\" penalization level predict(cv_results, newdata = freeny[1:5, 2:5]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 8.795162 8.807070 8.824535 8.842175 8.882970   # Extract the residuals at the \"best\" penalization level residuals(cv_results) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.002801588 -0.015699794 -0.009674689 #> 1963 -0.029165027  0.024539756  0.012794692  0.013400637 #> 1964 -0.014177641  0.006727561 -0.001786569  0.012847301 #> 1965 -0.026214283 -0.003738367 -0.010414370 -0.002497682 #> 1966 -0.014953173  0.014923561  0.008857587  0.002955549 #> 1967 -0.002438578  0.011336771 -0.004817002 -0.002203485 #> 1968 -0.014622332 -0.016038521  0.004783442  0.009509723 #> 1969  0.008935501  0.025135859  0.011008309  0.028171208 #> 1970 -0.021812388  0.015300404 -0.005936032 -0.018484409 #> 1971 -0.011338929  0.005850584  0.003783641  0.007952774 # Extract the residuals at a more parsimonious penalization level residuals(cv_results, lambda = \"1.5-se\") #>               Qtr1          Qtr2          Qtr3          Qtr4 #> 1962               -0.0133914763 -0.0253025812 -0.0180073467 #> 1963 -0.0375085950  0.0197904960  0.0063674275  0.0071084444 #> 1964 -0.0199445085  0.0028559476 -0.0059886764  0.0089958759 #> 1965 -0.0300906249 -0.0052788324 -0.0128420949 -0.0036364879 #> 1966 -0.0166186511  0.0137904162  0.0073750895  0.0016121587 #> 1967 -0.0021618673  0.0116385091 -0.0046227113  0.0002016235 #> 1968 -0.0117185912 -0.0127504673  0.0088610048  0.0131305182 #> 1969  0.0114094522  0.0285319822  0.0147126545  0.0325247699 #> 1970 -0.0158575338  0.0222103711  0.0004937030 -0.0127802705 #> 1971 -0.0035765461  0.0133307584  0.0117139290  0.0154227310"},{"path":"/reference/residuals.pense_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Residuals — residuals.pense_fit","title":"Extract Residuals — residuals.pense_fit","text":"Extract residuals PENSE (LS-EN) regularization path fitted pense(), regmest() elnet().","code":""},{"path":"/reference/residuals.pense_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Residuals — residuals.pense_fit","text":"","code":"# S3 method for class 'pense_fit' residuals(object, alpha = NULL, lambda, ...)"},{"path":"/reference/residuals.pense_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Residuals — residuals.pense_fit","text":"object PENSE regularization path extract residuals . alpha Either single number NULL (default). given, fits given alpha value considered. object fit multiple alpha values, value provided, first value object$alpha used warning. lambda single number penalty level. ... currently used.","code":""},{"path":"/reference/residuals.pense_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Residuals — residuals.pense_fit","text":"numeric vector residuals given penalization level.","code":""},{"path":[]},{"path":"/reference/residuals.pense_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Residuals — residuals.pense_fit","text":"","code":"# Compute the LS-EN regularization path for Freeny's revenue data # (see ?freeny) data(freeny) x <- as.matrix(freeny[ , 2:5])  regpath <- elnet(x, freeny$y, alpha = 0.75)  # Predict the response using a specific penalization level predict(regpath, newdata = freeny[1:5, 2:5],         lambda = regpath$lambda[[1]][[10]]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 9.071638 9.075877 9.082341 9.091051 9.103643   # Extract the residuals at a certain penalization level residuals(regpath, lambda = regpath$lambda[[1]][[5]]) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.396169224 -0.398919454 -0.378220695 #> 1963 -0.384593892 -0.294338479 -0.278030430 -0.259140970 #> 1964 -0.265543684 -0.219857551 -0.206023748 -0.173545125 #> 1965 -0.192086388 -0.146253918 -0.133786817 -0.096671191 #> 1966 -0.090347926 -0.044601264 -0.027307275 -0.015531362 #> 1967  0.006739697  0.037235389  0.038869333  0.071407419 #> 1968  0.086275062  0.102036759  0.141643505  0.167310507 #> 1969  0.175437660  0.211500792  0.222942549  0.260029329 #> 1970  0.247870647  0.296181577  0.294470169  0.278217170 #> 1971  0.306686546  0.334684047  0.353729154  0.367702082  # Select penalization level via cross-validation set.seed(123) cv_results <- elnet_cv(x, freeny$y, alpha = 0.5,                        cv_repl = 10, cv_k = 4)  # Predict the response using the \"best\" penalization level predict(cv_results, newdata = freeny[1:5, 2:5]) #>  1962.25   1962.5  1962.75     1963  1963.25  #> 8.795162 8.807070 8.824535 8.842175 8.882970   # Extract the residuals at the \"best\" penalization level residuals(cv_results) #>              Qtr1         Qtr2         Qtr3         Qtr4 #> 1962              -0.002801588 -0.015699794 -0.009674689 #> 1963 -0.029165027  0.024539756  0.012794692  0.013400637 #> 1964 -0.014177641  0.006727561 -0.001786569  0.012847301 #> 1965 -0.026214283 -0.003738367 -0.010414370 -0.002497682 #> 1966 -0.014953173  0.014923561  0.008857587  0.002955549 #> 1967 -0.002438578  0.011336771 -0.004817002 -0.002203485 #> 1968 -0.014622332 -0.016038521  0.004783442  0.009509723 #> 1969  0.008935501  0.025135859  0.011008309  0.028171208 #> 1970 -0.021812388  0.015300404 -0.005936032 -0.018484409 #> 1971 -0.011338929  0.005850584  0.003783641  0.007952774 # Extract the residuals at a more parsimonious penalization level residuals(cv_results, lambda = \"1.5-se\") #>               Qtr1          Qtr2          Qtr3          Qtr4 #> 1962               -0.0133914763 -0.0253025812 -0.0180073467 #> 1963 -0.0375085950  0.0197904960  0.0063674275  0.0071084444 #> 1964 -0.0199445085  0.0028559476 -0.0059886764  0.0089958759 #> 1965 -0.0300906249 -0.0052788324 -0.0128420949 -0.0036364879 #> 1966 -0.0166186511  0.0137904162  0.0073750895  0.0016121587 #> 1967 -0.0021618673  0.0116385091 -0.0046227113  0.0002016235 #> 1968 -0.0117185912 -0.0127504673  0.0088610048  0.0131305182 #> 1969  0.0114094522  0.0285319822  0.0147126545  0.0325247699 #> 1970 -0.0158575338  0.0222103711  0.0004937030 -0.0127802705 #> 1971 -0.0035765461  0.0133307584  0.0117139290  0.0154227310"},{"path":"/reference/rho-tuning-constants.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Constant for Consistency for the M-Scale and for Efficiency for the M-estimate of Location — consistency_const","title":"Get the Constant for Consistency for the M-Scale and for Efficiency for the M-estimate of Location — consistency_const","text":"Returns tuning constants required achieve desired breakdown point efficiency Normal model.","code":""},{"path":"/reference/rho-tuning-constants.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Constant for Consistency for the M-Scale and for Efficiency for the M-estimate of Location — consistency_const","text":"","code":"consistency_const(delta, rho, eps = sqrt(.Machine$double.eps))  efficiency_const(eff, rho, eps = sqrt(.Machine$double.eps))"},{"path":"/reference/rho-tuning-constants.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Constant for Consistency for the M-Scale and for Efficiency for the M-estimate of Location — consistency_const","text":"delta desired breakdown point (0 0.5) rho name chosen \\(\\rho\\) function. See rho_function() list supported functions. eps numerical tolerance level equality comparisons eff desired asymptotic efficiency (0.1 0.99).","code":""},{"path":"/reference/rho-tuning-constants.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Constant for Consistency for the M-Scale and for Efficiency for the M-estimate of Location — consistency_const","text":"consistency constant","code":""},{"path":[]},{"path":"/reference/rho_function.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Rho Functions — rho_function","title":"List Available Rho Functions — rho_function","text":"List Available Rho Functions","code":""},{"path":"/reference/rho_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Rho Functions — rho_function","text":"","code":"rho_function(rho, convex_ok = TRUE)"},{"path":"/reference/rho_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Rho Functions — rho_function","text":"rho name \\(\\rho\\) function check existence. convex_ok convex \\(\\rho\\) function acceptable .","code":""},{"path":"/reference/rho_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Rho Functions — rho_function","text":"rho missing returns vector supported \\(\\rho\\) function names, otherwise internal integer representation \\(\\rho\\) function.","code":""},{"path":[]},{"path":"/reference/starting_point.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Starting Points for the PENSE Algorithm — starting_point","title":"Create Starting Points for the PENSE Algorithm — starting_point","text":"Create starting point starting PENSE algorithm pense(). Multiple starting points can created combining starting points via c(starting_point_1, starting_point_2, ...).","code":""},{"path":"/reference/starting_point.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Starting Points for the PENSE Algorithm — starting_point","text":"","code":"starting_point(beta, intercept, lambda, alpha)  as_starting_point(object, specific = FALSE, ...)  # S3 method for class 'enpy_starting_points' as_starting_point(object, specific = FALSE, ...)  # S3 method for class 'pense_fit' as_starting_point(object, specific = FALSE, alpha, lambda, ...)  # S3 method for class 'pense_cvfit' as_starting_point(   object,   specific = FALSE,   alpha,   lambda = c(\"min\", \"se\"),   se_mult = 1,   ... )"},{"path":"/reference/starting_point.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Starting Points for the PENSE Algorithm — starting_point","text":"beta beta coefficients starting point. Can numeric vector, sparse vector class dsparseVector, sparse matrix class dgCMatrix single column. intercept intercept coefficient starting point. lambda optionally either string specifying penalty level use (\"min\" \"se\") numeric vector penalty levels extract object. Penalization levels present object ignored warning. NULL, estimates object extracted. numeric vector, alpha must given single number. alpha optional value alpha hyper-parameter. given, estimates matching alpha values extracted. Values present object ignored warning. object object estimates use starting points. specific whether estimates used starting points penalization level computed . Defaults using estimates starting points penalization levels. ... arguments passed methods. se_mult lambda = \"se\", multiple standard errors tolerate.","code":""},{"path":"/reference/starting_point.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Starting Points for the PENSE Algorithm — starting_point","text":"object type starting_points used starting point pense().","code":""},{"path":"/reference/starting_point.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Starting Points for the PENSE Algorithm — starting_point","text":"starting points can either shared, .e., used every penalization level PENSE estimates computed , specific one penalization level. create specific starting point, provide penalization parameters lambda alpha. lambda alpha missing, shared starting point created. Shared specific starting points can combined single list starting points, pense() handling correctly. Note specific starting points lead lambda value added grid penalization levels. See pense() details. Starting points computed via enpy_initial_estimates() default shared starting points can transformed specific starting points via as_starting_point(..., specific = TRUE). creating starting points cross-validated fits, possible extract estimate best CV performance (lambda = \"min\"), estimate CV performance statistically indistinguishable best performance (lambda = \"se\"). determined estimate prediction performance within se_mult * cv_se best model.","code":""},{"path":[]},{"path":"/reference/summary.pense_cvfit.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize Cross-Validated PENSE Fit — summary.pense_cvfit","title":"Summarize Cross-Validated PENSE Fit — summary.pense_cvfit","text":"lambda = \"se\" object contains fitted estimates every penalization level sequence, extract coefficients parsimonious model prediction performance statistically indistinguishable best model. determined model prediction performance within se_mult * cv_se best model.","code":""},{"path":"/reference/summary.pense_cvfit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize Cross-Validated PENSE Fit — summary.pense_cvfit","text":"","code":"# S3 method for class 'pense_cvfit' summary(object, alpha, lambda = \"min\", se_mult = 1, ...)  # S3 method for class 'pense_cvfit' print(x, alpha, lambda = \"min\", se_mult = 1, ...)"},{"path":"/reference/summary.pense_cvfit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize Cross-Validated PENSE Fit — summary.pense_cvfit","text":"object, x (adaptive) PENSE fit cross-validation information. alpha Either single number missing. given, fits given alpha value considered. lambda numeric value object fit multiple alpha values, parameter alpha must missing. lambda either string specifying penalty level use (\"min\", \"se\", \"{x}-se\") single numeric value penalty parameter. See details. se_mult lambda = \"se\", multiple standard errors tolerate. ... ignored.","code":""},{"path":[]},{"path":"/reference/tau_size.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the Tau-Scale of Centered Values — tau_size","title":"Compute the Tau-Scale of Centered Values — tau_size","text":"Compute \\(\\tau\\)-scale without centering values.","code":""},{"path":"/reference/tau_size.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the Tau-Scale of Centered Values — tau_size","text":"","code":"tau_size(x)"},{"path":"/reference/tau_size.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the Tau-Scale of Centered Values — tau_size","text":"x numeric values. Missing values verbosely ignored.","code":""},{"path":"/reference/tau_size.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the Tau-Scale of Centered Values — tau_size","text":"\\(\\tau\\) estimate scale centered values.","code":""},{"path":[]},{"path":"/news/index.html","id":"pense-252","dir":"Changelog","previous_headings":"","what":"pense 2.5.2","title":"pense 2.5.2","text":"CRAN release: 2026-01-27 Fix division value close 0 certain LAPACK implementations, leading residuals without variation failure RIS-CV.","code":""},{"path":"/news/index.html","id":"pense-250","dir":"Changelog","previous_headings":"","what":"pense 2.5.0","title":"pense 2.5.0","text":"CRAN release: 2026-01-13 Use Robust Information Sharing Cross-Validation (RIS-CV) default pense_cv() adapense_cv(). Details RIS-CV can found Kepplinger & Wei (2025). Add support optimal rho function (Maronna et al. 2018, Section 5.8.1), using polynomial approximation robustbase package. Fix undefined behavior identified extended CRAN checks.","code":""},{"path":"/news/index.html","id":"pense-222","dir":"Changelog","previous_headings":"","what":"pense 2.2.2","title":"pense 2.2.2","text":"CRAN release: 2024-07-27 Fix build warnings GCC 14.1.0 CRAN","code":""},{"path":"/news/index.html","id":"pense-221","dir":"Changelog","previous_headings":"","what":"pense 2.2.1","title":"pense 2.2.1","text":"Fix build error Windows using GCC version < 9.","code":""},{"path":"/news/index.html","id":"pense-220","dir":"Changelog","previous_headings":"","what":"pense 2.2.0","title":"pense 2.2.0","text":"CRAN release: 2023-02-07 Fix bug argument max_solutions used correctly. Add new numerical coordinate-descent algorithms LS-EN (en_cd_options()). Revamp regularization path starting points used shared. Share starting points across CV folds replications. Address changes upcoming Rcpp release.","code":""},{"path":"/news/index.html","id":"pense-210","dir":"Changelog","previous_headings":"","what":"pense 2.1.0","title":"pense 2.1.0","text":"CRAN release: 2021-07-07 Penalty loadings now applied L1 L2 parts EN penalty. lead different results adaptive PENSE adaptive estimators fitted alpha < 1! PENSE regularized M-estimators now accept multiple alpha values automatic hyper-parameter selection also choose best alpha value. New support specifying general “1-SE” rules penalization level string. methods support lambda = \"min\" extract best fit, also support syntax lambda = \"{m}-se\" extract parsimonious fit within m standard-errors best fit. Adaptively choose actual breakdown point based number observations. chosen breakdown point close user-specified breakdown point, avoids numerical instabilities S-loss excessive computation time caused instabilities. Simplify DAL algorithm fully rely linear algebra routines BLAS/LAPACK library linked R. improve speed DAL algorithm, optimized BLAS/LAPACK libraries recommended. Fix memory issues edge-cases OpenMP problems Intel compilers","code":""},{"path":"/news/index.html","id":"pense-203","dir":"Changelog","previous_headings":"","what":"pense 2.0.3","title":"pense 2.0.3","text":"CRAN release: 2021-04-14 Fix bug causing PENSE-Ridge, .e., pense(..., alpha = 0), take long time compute. Fix compilation error RHEL due error autoconf script. Fix problems prediction_performance() related non-standard evaluation objects. Also return standardized coefficients std_beta std_intercept. # pense 2.0.2 Fix mishandling response variables robust scale 0, e.g., 0-inflated responses responses 50% identical values. # pense 2.0.1 Add new functions compute adaptive PENSE estimates (adapense() adapense_cv()). Functions fitting model (pense(), adapense(), regmest(), etc.) estimating prediction performance via cross-validation anymore. can now done using corresponding functions pense_cv(), adapense_cv(), . New function prediction_performance() summarize prediction performance several fits. plot(), coef(), summary(), predict() methods cross-validated fits also implement “one-standard-error rule” (“1” adjustable user). Decrease computation time problems. New ADMM algorithm (weighted) elastic net problems many observations many predictors. new algorithm can selected en_admm_options(). Argument correct pense(), pensem(), coef(), etc., supported anymore ignored warning. estimates now uncorrected (.e., correct=FALSE previous versions package). pensem() now called pensem_cv(). initest_options() replaced enpy_options() using better naming arguments. en_options_aug_lars() en_options_dal() replaced en_lars_options() en_dal_options() consistent naming. pense_options() mstep_options() superseded mm_algorithm_options() arguments specified calls pense() companions. enpy() replaced enpy_initial_estimates() different default argument values. Deprecated functions can still used (now) warning.","code":""},{"path":"/news/index.html","id":"pense-129","dir":"Changelog","previous_headings":"","what":"pense 1.2.9","title":"pense 1.2.9","text":"CRAN release: 2020-02-09 Fix LTO warnings reported CRAN checks Update autoconf script address deprecation warnings r-devel.","code":""},{"path":"/news/index.html","id":"pense-125","dir":"Changelog","previous_headings":"","what":"pense 1.2.5","title":"pense 1.2.5","text":"CRAN release: 2019-06-08 Fix compatibility BLAS/LAPACK prototypes RcppArmadillo 0.9.500.","code":""},{"path":"/news/index.html","id":"pense-124","dir":"Changelog","previous_headings":"","what":"pense 1.2.4","title":"pense 1.2.4","text":"CRAN release: 2019-04-27 Fix autoconf script.","code":""},{"path":"/news/index.html","id":"pense-121","dir":"Changelog","previous_headings":"","what":"pense 1.2.1","title":"pense 1.2.1","text":"CRAN release: 2019-01-17 Prepare changes upcoming Rcpp (make compatible STRICT_R_HEADERS) Fix bug computing PSCs using augmented ridge algorithm EN.","code":""},{"path":"/news/index.html","id":"pense-120","dir":"Changelog","previous_headings":"","what":"pense 1.2.0","title":"pense 1.2.0","text":"CRAN release: 2018-03-11 Changed internal scaling regularization parameter pense pensem. Note: lambda values release previous releases! Fixed bug standardizing predictor variables MAD 0 (thanks @hadjipantelis reporting). maximum value regularization parameter lambda now chosen exactly. Fixed bug computing “exact” principal sensitivity components. # pense 1.0.8 Fix error robustbase-0.92-8 reported Martin Maechler. Fix undefined behavior C++ code resulting build error Solaris (x86). Fix predict() function pensem objects computed fitted pense object. Always use delta cc specified pense_options() initial estimator. Remove delta cc arguments initest_options() instead add enpy(). Add measure prediction performance (resid_size) obj$cv_lambda_grid, obj class pense pensem. # pense 1.0.6: Initial stable release package.","code":""}]
